{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17857b45",
   "metadata": {},
   "source": [
    "# Lab 8: Longitudinal fMRI ROI Analysis and Power in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa370756",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    " - **Apply paired t-tests** to longitudinal fMRI ROI beta weights (within-subject changes over time) and interpret results.\n",
    "\n",
    " - **Apply independent t-tests** to compare brain activation between groups (e.g. high vs low on a behavior) and examine assumptions (normality, variance) for using Welch’s t-test.\n",
    "\n",
    " - **Calculate effect sizes (Cohen’s d)** for paired and independent t-tests, and distinguish between statistically significant and practically meaningful differences.\n",
    "\n",
    " - **Perform power simulations and calculations** to understand Type I error (false positives), power for a given effect size, and required sample sizes for desired power levels.\n",
    "\n",
    " - **Reinforce concepts of fMRI data**: connecting percent signal change (PSC) to GLM beta weights, and interpreting ROI beta differences in the context of brain function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4560975",
   "metadata": {},
   "source": [
    "## Background: Percent Signal Change (PSC) vs. Beta Weights\n",
    "\n",
    "In fMRI analysis, we use a simple statistical tool called a **General Linear Model (GLM)** to describe how the BOLD signal changes over time. You can think of a GLM like a smart line-fitting tool (similar to regression) that asks: *“How much does this brain region’s signal tend to go up or down when a certain type of image or event is on the screen, compared to baseline?”* \n",
    "\n",
    "The GLM gives us **beta weights** for each condition. These beta weights represent how much a region’s activity increases (or decreases) in a given condition, often expressed in the same units as the raw signal (which can be arbitrary). To make these changes interpretable, we often talk about changes as a **Percent Signal Change (PSC)** – how much the BOLD signal rises or falls relative to a baseline, in percentage terms. \n",
    "\n",
    "For example, if a region’s BOLD signal is 100 (arbitrary units) at baseline and 112 during a task, the PSC = ((112 − 100) / 100) × 100% = +12%. The GLM beta for that condition essentially captures this difference (here, roughly +12 in raw units corresponding to +12% PSC).\n",
    "\n",
    "Let’s walk through a simple demonstration. We simulate a time series with a stable baseline of 100 units, and then an “event” period where the signal increases by 5 units (which is a 5% increase over baseline):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf66e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Simulate a baseline signal of 100 for 120 time points\n",
    "time = np.arange(0, 120)\n",
    "signal = np.ones_like(time) * 100.0\n",
    "\n",
    "# Simulate an event from t=50 to t=69 where signal increases by 5 units (5% of 100)\n",
    "signal[50:70] += 5.0\n",
    "\n",
    "# Calculate baseline and event means\n",
    "baseline_mean = signal[:50].mean()\n",
    "event_mean = signal[50:70].mean()\n",
    "psc = (event_mean - baseline_mean) / baseline_mean * 100\n",
    "print(f\"Baseline mean = {baseline_mean:.1f}, Event mean = {event_mean:.1f}, PSC = {psc:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e5881",
   "metadata": {},
   "source": [
    "Running the above, we get:\n",
    "\n",
    "The baseline is 100.0, event period mean 105.0, yielding a +5.0% signal change. Next, we can fit a simple GLM with an intercept (baseline) and an event regressor to see the estimated beta weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regressor that is 1 during the event and 0 otherwise\n",
    "event_regressor = np.zeros_like(time)\n",
    "event_regressor[50:70] = 1\n",
    "\n",
    "# Design matrix: intercept + event regressor\n",
    "X = np.column_stack([np.ones_like(time), event_regressor])\n",
    "\n",
    "# Solve OLS for beta estimates\n",
    "beta_hat = np.linalg.inv(X.T @ X) @ X.T @ signal\n",
    "intercept_est, event_beta_est = beta_hat\n",
    "print(f\"Estimated intercept = {intercept_est:.1f}, event beta = {event_beta_est:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b01f88",
   "metadata": {},
   "source": [
    "The model finds two key numbers: an intercept of about 100 (the baseline level) and an event beta of about +5. **That beta of 5 means the signal goes up by 5 units during the event, which in this example is a 5% increase over the baseline of 100.** More generally, the beta tells us how big the change in the BOLD signal is (ΔBOLD), and **PSC is that same change** (calculated by dividing the condition by the baseline and expressed as a percent). In the ROI analyses below, the beta weights already tell us how strong the activation is, so we interpret how they differ across conditions or time points. For example, a beta change from 0.2 to 0.3 would represent a small increase in activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c38ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to create the plot illustrating the relationship between PSC and beta\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated time axis (TRs)\n",
    "t = np.arange(0, 121)  # 0–120\n",
    "\n",
    "# Baseline and event parameters\n",
    "baseline = 100\n",
    "event_increase = 5         # +5 units = 5% increase\n",
    "event_start = 60           # TR where event starts\n",
    "event_end = 120            # TR where event ends\n",
    "\n",
    "# Build the signal: baseline, then a smooth step up to baseline + 5\n",
    "signal = np.full_like(t, baseline, dtype=float)\n",
    "\n",
    "# Simple smooth transition around event_start (optional, just for aesthetics)\n",
    "ramp_len = 5  # number of TRs over which signal ramps up\n",
    "ramp_start = event_start\n",
    "ramp_end = event_start + ramp_len\n",
    "\n",
    "signal[ramp_start:ramp_end] = np.linspace(baseline, baseline + event_increase, ramp_len)\n",
    "signal[ramp_end:event_end + 1] = baseline + event_increase\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Orange activation band\n",
    "plt.axvspan(event_start, event_end, color='orange', alpha=0.2)\n",
    "\n",
    "# Time series line\n",
    "plt.plot(t, signal, linewidth=3)\n",
    "\n",
    "# Axis labels and limits\n",
    "plt.xlabel(\"Time (TRs)\")\n",
    "plt.ylabel(\"Percent signal (arbitrary units)\")\n",
    "plt.ylim(90, 110)\n",
    "\n",
    "# Title (two-line, like your slide)\n",
    "plt.title(\"Simulated fMRI Time Series: Baseline vs Activation\\n(5 percent PSC)\")\n",
    "\n",
    "# Text annotations\n",
    "plt.text(15, baseline + 1.0, \"Baseline = 100\", fontsize=10)\n",
    "plt.text(75, baseline + event_increase + 1.0, \"Event mean = 105\", fontsize=10)\n",
    "plt.text(80, baseline + 0.5, \"Activation period:\\n+5 units\", fontsize=10)\n",
    "\n",
    "# PSC and GLM notes at bottom\n",
    "plt.text(60, 92,\n",
    "         \"Percent signal change: 5 percent (105 vs 100)\\n\"\n",
    "         \"GLM intercept ≈ 100,  beta ≈ 5\",\n",
    "         fontsize=9, ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ee6410",
   "metadata": {},
   "source": [
    "*Figure 1.* Simulated fMRI time series with a flat baseline around 100 and an activation period (orange band) where the signal rises to about 105. In this simple model, the GLM intercept (~100) is the baseline level and the beta (~5) is the size of the bump above baseline. That same bump is a 5% percent signal change (PSC) because 5 / 100 × 100% = 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f693b",
   "metadata": {},
   "source": [
    "**How to read beta vs percent signal change (PSC)**\n",
    "\n",
    "- In this lab, **beta ($\\beta$)** is the change in the BOLD signal **relative to baseline**, in the same units as the raw signal.\n",
    "- **PSC** is that **same change**, but written as a **percent of the baseline**:  \n",
    "\n",
    "  $$\n",
    "  \\text{PSC} = \\frac{\\beta}{\\text{baseline}} \\times 100\\%.\n",
    "  $$\n",
    "\n",
    "- So if the baseline is 100 and $\\beta = 5$, that corresponds to a 5% increase.  \n",
    "- For the ROI betas we analyze below, you can think of $\\beta$ as “how strongly this region responds compared to baseline.” A change from $\\beta = 0.2$ to $\\beta = 0.3$ would be a small increase in activation.\n",
    "\n",
    "*(More advanced note: in real fMRI pipelines, the exact scaling of $\\beta$ depends on how the data and regressors are preprocessed, but the core idea is the same — $\\beta$ measures how big the response is relative to baseline, and PSC is that response expressed in percent.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d672aa15",
   "metadata": {},
   "source": [
    "In the following activities, we will analyze synthetic fMRI beta-weight data (ABCD style) from a monetary incentive delay (MID) task. Beta values represent activation magnitudes (in arbitrary units) for certain conditions (e.g., reward anticipation, reward outcome) in specific ROIs (regions of interest). All analyses will use exact variable names from the data. We’ll start with paired t-tests for longitudinal within-subject changes, then independent t-tests for group differences, and finally examine statistical power and sample size considerations.\n",
    "\n",
    "**Dataset:** We use the same dataset from Labs 6 and 7. The dataset ('labs/data/L1/ABCD_synthetic.csv') contains longitudinal MID task data for N=11,604 participants at approximately age 16 and age 21. Key variables include ROI beta weights for the MID task contrasts: for example, `mr_y_tfmri__mid__arvn__aseg__ab__lh_beta` is the left nucleus accumbens (NAcc) beta for anticipation of reward vs neutral, and `...__rh_beta` the right; similarly `...__cd__` and `...__pt__` correspond to caudate and putamen (dorsal striatum). The “aseg” labels come from FreeSurfer’s automated segmentation (ROIs in subcortical atlas). We will focus on bilateral NAcc (averaging left and right) as our ventral striatum ROI, and occasionally compare it to bilateral caudate/putamen (dorsal striatum). The outcome phase contrast (rpvnf) is an NAcc beta for reward outcome (e.g., perhaps reward receipt vs no-feedback). Behavioral and demographic variables (e.g. social media use, substance use, etc.) are also available at age 21 for grouping in independent t-tests.\n",
    "\n",
    "Before diving into hypothesis tests, let’s define how we’ll calculate bilateral ROI measures and effect sizes:\n",
    "\n",
    "- **Bilateral ROI beta** = average of left and right ROI betas.  \n",
    "- **Paired t-test effect size (Cohen’s d for paired samples)** = mean of the within-subject differences divided by the standard deviation of those differences.  \n",
    "- **Independent t-test effect size (Cohen’s d for two groups)** = difference in group means divided by the pooled standard deviation.  \n",
    "\n",
    "We will use Welch’s t-test for unequal variances by default, but still compute a pooled SD for effect size estimation. We will also report 95% confidence intervals (CI) for differences when relevant. A 95% CI that does not include 0 corresponds to p < 0.05, indicating a statistically significant difference.\n",
    "\n",
    "With these foundations, let’s proceed to the analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10d51b",
   "metadata": {},
   "source": [
    "## Activity 1: Paired t-Tests with Longitudinal fMRI Beta Weights\n",
    "\n",
    "In Activity 1, we analyze how brain activation in certain ROIs changes from age 16 to 21 within the same individuals. We use **paired t-tests** for these within-subject comparisons. A significant result indicates a reliable average change in activation over time. We will also examine whether those changes are large or small in magnitude (effect size) and reflect on their practical meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f80e49",
   "metadata": {},
   "source": [
    "### Demo 1a: Bilateral NAcc Reward Anticipation Beta – Age 16 vs 21\n",
    "\n",
    "First, we test whether activation in the nucleus accumbens (NAcc) during reward anticipation has changed from age 16 to 21. Reward anticipation is the “waiting and hoping” period when the participant sees a cue that a reward *might* be coming but does not yet know the outcome. The NAcc is a core reward region, so it’s interesting to see if adolescents vs young adults show different levels of anticipatory reward activity.\n",
    "\n",
    "**Workflow:**\n",
    "1. Compute bilateral NAcc anticipation betas at 16 and 21 for each subject (average of left and right NAcc arvn betas).  \n",
    "2. Examine descriptive statistics (mean and SD at each age).  \n",
    "3. Plot the distribution of the within-subject differences.  \n",
    "4. Perform a paired t-test.  \n",
    "5. Compute Cohen’s d for the change.  \n",
    "6. Interpret the result.\n",
    "\n",
    "Let’s do this step by step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Load data and compute bilateral NAcc anticipation beta for each wave\n",
    "df = pd.read_csv('data/L6L10/ABCD_synthetic.csv')\n",
    "\n",
    "df['NAcc_ant_bilateral'] = (\n",
    "    df['mr_y_tfmri__mid__arvn__aseg__ab__lh_beta'] +\n",
    "    df['mr_y_tfmri__mid__arvn__aseg__ab__rh_beta']\n",
    ") / 2\n",
    "\n",
    "# Pivot data to have one row per subject with columns for age 16 and 21 values\n",
    "nacc_wide = df.pivot(index='subject_id', columns='wave', values='NAcc_ant_bilateral')\n",
    "nacc_wide.columns = ['beta_16', 'beta_21']  # rename columns for clarity\n",
    "\n",
    "# Drop any subjects missing either timepoint (if any)\n",
    "nacc_wide = nacc_wide.dropna()\n",
    "\n",
    "# Calculate descriptive stats\n",
    "mean16 = nacc_wide['beta_16'].mean(); sd16 = nacc_wide['beta_16'].std()\n",
    "mean21 = nacc_wide['beta_21'].mean(); sd21 = nacc_wide['beta_21'].std()\n",
    "mean_diff = (nacc_wide['beta_21'] - nacc_wide['beta_16']).mean()\n",
    "sd_diff = (nacc_wide['beta_21'] - nacc_wide['beta_16']).std()\n",
    "\n",
    "print(f\"NAcc anticipation beta @16: mean={mean16:.3f}, SD={sd16:.3f}\")\n",
    "print(f\"NAcc anticipation beta @21: mean={mean21:.3f}, SD={sd21:.3f}\")\n",
    "print(f\"Mean within-subject change = {mean_diff:.3f} (SD of changes={sd_diff:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e5ccba",
   "metadata": {},
   "source": [
    "Suppose we get output (values are illustrative):\n",
    "\n",
    "```\n",
    "NAcc anticipation beta @16: mean=0.204, SD=0.987  \n",
    "NAcc anticipation beta @21: mean=0.213, SD=0.995  \n",
    "Mean within-subject change = 0.009 (SD of changes=0.678)\n",
    "```\n",
    "\n",
    "So on average, NAcc anticipation activation increased very slightly (+0.009 in beta units) from 16 to 21. This difference is extremely small relative to the variability (SD of individual changes ~0.68). We suspect this change will not be practically significant, though with a large N it might turn up statistically significant.\n",
    "\n",
    "Let’s visualize the distribution of individual changes (beta at 21 minus beta at 16):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66092027",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "diff = nacc_wide['beta_21'] - nacc_wide['beta_16']\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(diff, bins=30, color='skyblue', edgecolor='gray')\n",
    "plt.axvline(diff.mean(), color='red', linestyle='--', label=f\"Mean change = {diff.mean():.3f}\")\n",
    "plt.xlabel('NAcc Beta (21 – 16)')\n",
    "plt.ylabel('Number of Subjects')\n",
    "plt.title('Distribution of NAcc Reward Anticipation Change (16→21)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8bf84",
   "metadata": {},
   "source": [
    "The distribution of within-subject changes in bilateral NAcc reward anticipation beta from age 16 to 21 is centered near zero. Most participants cluster around zero change; some increased, some decreased, but there is no large shift in either direction.\n",
    "\n",
    "Now, the paired t-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired t-test\n",
    "t_stat, p_val = stats.ttest_rel(nacc_wide['beta_21'], nacc_wide['beta_16'])\n",
    "df_deg = len(nacc_wide) - 1  # degrees of freedom\n",
    "# Cohen's d for paired: mean change / std of change\n",
    "cohen_d = mean_diff / sd_diff\n",
    "\n",
    "print(f\"Paired t-test: t({df_deg}) = {t_stat:.2f}, p = {p_val:.3g}\")\n",
    "print(f\"Cohen's d (paired) = {cohen_d:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0fdb1e",
   "metadata": {},
   "source": [
    "**Quick check of our actual output**\n",
    "\n",
    "In the run above, the paired t-test gave something like:\n",
    "\n",
    "`Paired t-test: t(4268) = 0.00, p = 0.998`  \n",
    "`Cohen's d (paired) = 0.00`\n",
    "\n",
    "This means that, in this sample, there is essentially **no difference** in NAcc anticipation beta between age 16 and age 21. The mean change is basically zero, the p-value is very large (nowhere near 0.05), and the effect size d ≈ 0 indicates no meaningful change in standard deviation units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c668961",
   "metadata": {},
   "source": [
    "#### Hypothetical Situation: Meaningful Effect Sizes\n",
    "\n",
    "Now let’s imagine a **different** (hypothetical) test of the same variables that *did* produce a statistically significant result:\n",
    "\n",
    "```\n",
    "Paired t-test: t(11603) = 2.10, p = 0.036  \n",
    "Cohen's d (paired) = 0.01\n",
    "```\n",
    "\n",
    "There is a statistically significant increase in NAcc anticipation activation from 16 to 21 (p ≈ 0.036). However, the effect size is d ≈ 0.01, which is extremely small – essentially negligible. The 95% confidence interval on the mean change would be something like [0.0005, 0.018] (just barely above 0). In practical terms, a change of 0.009 in beta (on a baseline of ~0.2) is trivial. This is a great example of how a large sample can detect minuscule differences that have little practical meaning. We should reflect: Is a 0.01 SD change meaningful? Probably not – it’s essentially no change in real-world terms, even though it’s statistically non-zero. It’s important to note such tiny effects might arise from slight scanner or sample differences over time rather than true developmental change.\n",
    "\n",
    "**Interpretation:** On average, adolescents and young adults have very similar NAcc reactivity during reward anticipation. Any developmental increase in this sample is so small as to be negligible. So, while the test is formally significant, we would report that NAcc anticipation response remained stable from age 16 to 21 (no meaningful change)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3efe09c",
   "metadata": {},
   "source": [
    "### Demo 1b: Bilateral NAcc Outcome/Feedback Beta – Age 16 vs 21\n",
    "\n",
    "Next, we examine the NAcc activation during **reward feedback** (outcome) at 16 vs 21. Reward feedback is the moment when the task tells the participant whether they actually won or received the reward (e.g., “You got the money!”). Perhaps adolescents react more strongly to getting a reward, or maybe with age the outcome is less exciting relative to the anticipation. We follow the same workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bilateral NAcc reward outcome beta for each wave and pivot\n",
    "df['NAcc_out_bilateral'] = (\n",
    "    df['mr_y_tfmri__mid__rpvnf__aseg__ab__lh_beta'] +\n",
    "    df['mr_y_tfmri__mid__rpvnf__aseg__ab__rh_beta']\n",
    ") / 2\n",
    "\n",
    "nacc_out_wide = df.pivot(index='subject_id', columns='wave', values='NAcc_out_bilateral').dropna()\n",
    "nacc_out_wide.columns = ['beta_out_16', 'beta_out_21']\n",
    "\n",
    "# Descriptive stats\n",
    "mean16 = nacc_out_wide['beta_out_16'].mean(); mean21 = nacc_out_wide['beta_out_21'].mean()\n",
    "mean_diff = (nacc_out_wide['beta_out_21'] - nacc_out_wide['beta_out_16']).mean()\n",
    "print(f\"NAcc outcome beta @16 mean={mean16:.3f}, @21 mean={mean21:.3f}, mean change={mean_diff:.3f}\")\n",
    "\n",
    "# Paired t-test and effect size\n",
    "t_stat, p_val = stats.ttest_rel(nacc_out_wide['beta_out_21'], nacc_out_wide['beta_out_16'])\n",
    "d = mean_diff / (nacc_out_wide['beta_out_21'] - nacc_out_wide['beta_out_16']).std()\n",
    "print(f\"Paired t: t={t_stat:.2f}, p={p_val:.3g}, Cohen's d={d:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b488d",
   "metadata": {},
   "source": [
    "**Interpretation (actual output):**  \n",
    "For reward feedback, the mean NAcc beta increased slightly from age 16 (≈0.135) to age 21 (≈0.146), a change of about 0.011 units. Statistically, this difference is **significant** (t ≈ 2.78, p ≈ 0.005), but the effect size is **very small** (Cohen’s d ≈ 0.04). In practical terms, this suggests that NAcc response to reward outcome is extremely similar at 16 and 21, with only a tiny average increase that is likely not meaningful in real-world terms. The large sample size makes it easy to detect even this minuscule change.\n",
    "\n",
    "**Summary:** Our two demos suggest very little developmental change in NAcc activation for the MID task in this sample. For **reward anticipation**, the paired test showed essentially no change from 16 to 21 (d ≈ 0). For **reward outcome**, we saw a very small increase in NAcc beta by age 21 that was statistically significant but still tiny in size (d ≈ 0.04). For practical purposes, NAcc reward-related activation looks quite stable from mid-teens to early adulthood here. This is a good reminder to separate **statistical significance** from **practical significance**: with a huge sample, we can detect very small differences, but an effect size near 0–0.04 is likely not meaningful for brain function or behavior. Always look at effect sizes and context; a significant p-value is not the end of the story.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2818b",
   "metadata": {},
   "source": [
    "## Your Turn 1: Additional Paired t-Test Analyses (Choose ONE option)\n",
    "\n",
    "In Demo 1a and 1b we tested **within-subject change from age 16 to 21** in bilateral NAcc activation.  \n",
    "Now it’s your turn to run a similar *paired t-test* on a related question.\n",
    "\n",
    "Pick **ONE** of the options (A–D) below. For your chosen option you will:\n",
    "For your chosen option, follow this workflow:\n",
    "\n",
    "1. Compute the relevant measure at age 16 and 21 for each subject.\n",
    "2. Look at descriptive statistics (mean, SD, n at each age; mean change).\n",
    "3. **Plot 1 – Assumption check:** Make a histogram of the **within-subject change scores** (21 − 16). This is mainly to see if the changes look roughly symmetric/normal for the paired t-test.\n",
    "4. Run a **paired t-test** (16 vs 21).\n",
    "5. Compute a **95% confidence interval** for the mean change.\n",
    "6. Compute **Cohen’s d (paired)** as an effect size.\n",
    "7. **Plot 2 – Main result:** Make a simple bar plot showing the mean at age 16 and age 21 with error bars (standard error) is recommended.\n",
    "\n",
    "**Visualizations required (2 total):**\n",
    "\n",
    "- **Plot 1 (histogram of change scores)** — used as a *diagnostic* to check the normality assumption for the paired t-test.  \n",
    "- **Plot 2 (bar plot with error bars for 16 vs 21)** — used as the *main figure* to communicate your result (direction, size, and uncertainty) in a way that you could put on a slide or in a paper.\n",
    "\n",
    "For all options, we treat the questions as **two-sided** (we are testing “any change,” not predicting the direction).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Option A – Ventral vs Dorsal Striatum Anticipation Bias\n",
    "\n",
    "**Idea.** Compare ventral striatum (NAcc) vs dorsal striatum (caudate) during **reward anticipation**, and ask whether that bias changed with age.\n",
    "\n",
    "- **New variables you will create:**\n",
    "  - `Caud_ant_bilateral`  \n",
    "    - Mean of left and right caudate anticipation betas:  \n",
    "      `mr_y_tfmri__mid__arvn__aseg__cd__lh_beta`,  \n",
    "      `mr_y_tfmri__mid__arvn__aseg__cd__rh_beta`.\n",
    "  - `bias_ventral_vs_dorsal`  \n",
    "    - Ventral–dorsal anticipation bias at each wave:  \n",
    "      `bias_ventral_vs_dorsal = NAcc_ant_bilateral - Caud_ant_bilateral`.\n",
    "\n",
    "**Research question.**  \n",
    "Does the **ventral vs dorsal striatum anticipation bias** change from age 16 to 21?\n",
    "\n",
    "- **Null hypothesis (H₀):** The mean change in `bias_ventral_vs_dorsal` from 16 to 21 is 0.  \n",
    "- **Alternative hypothesis (H₁):** The mean change in `bias_ventral_vs_dorsal` from 16 to 21 is **not** 0.\n",
    "\n",
    "---\n",
    "\n",
    "### Option B – NAcc vs Putamen Anticipation Bias\n",
    "\n",
    "**Idea.** Same logic as Option A, but comparing NAcc to **putamen** (another dorsal striatum region).\n",
    "\n",
    "- **New variables you will create:**\n",
    "  - `Put_ant_bilateral`  \n",
    "    - Mean of left and right putamen anticipation betas:  \n",
    "      `mr_y_tfmri__mid__arvn__aseg__pt__lh_beta`,  \n",
    "      `mr_y_tfmri__mid__arvn__aseg__pt__rh_beta`.\n",
    "  - `bias_nacc_vs_put`  \n",
    "    - NAcc–putamen anticipation bias:  \n",
    "      `bias_nacc_vs_put = NAcc_ant_bilateral - Put_ant_bilateral`.\n",
    "\n",
    "**Research question.**  \n",
    "Does the **NAcc vs putamen anticipation bias** change from age 16 to 21?\n",
    "\n",
    "- **H₀:** The mean change in `bias_nacc_vs_put` from 16 to 21 is 0.  \n",
    "- **H₁:** The mean change in `bias_nacc_vs_put` from 16 to 21 is **not** 0.\n",
    "\n",
    "---\n",
    "\n",
    "### Option C – Anticipation vs Outcome Gap in NAcc\n",
    "\n",
    "**Idea.** Compare NAcc activation during **anticipation** vs **outcome/feedback**, and see if that **gap** changes with age.\n",
    "\n",
    "You already created:\n",
    "- `NAcc_ant_bilateral` – anticipation betas (average of left/right).\n",
    "- `NAcc_out_bilateral` – outcome betas (average of left/right).\n",
    "\n",
    "- **New variable you will create:**\n",
    "  - `NAcc_ant_out_gap`  \n",
    "    - Gap between anticipation and outcome at each age:  \n",
    "      `NAcc_ant_out_gap = NAcc_ant_bilateral - NAcc_out_bilateral`.\n",
    "\n",
    "**Research question.**  \n",
    "Does the **difference between anticipation and outcome activation** in NAcc change from age 16 to 21?\n",
    "\n",
    "- **H₀:** The mean change in `NAcc_ant_out_gap` from 16 to 21 is 0.  \n",
    "- **H₁:** The mean change in `NAcc_ant_out_gap` from 16 to 21 is **not** 0.\n",
    "\n",
    "---\n",
    "\n",
    "### Option D – Left vs Right NAcc Lateralization\n",
    "\n",
    "**Idea.** Check whether there is any **left–right asymmetry** (lateralization) in NAcc anticipation activation, and whether that lateralization changes with age.\n",
    "\n",
    "- **New variable you will create:**\n",
    "  - `NAcc_lat`  \n",
    "    - Left–right anticipation difference at each age:  \n",
    "      `NAcc_lat = mr_y_tfmri__mid__arvn__aseg__ab__lh_beta - mr_y_tfmri__mid__arvn__aseg__ab__rh_beta`.\n",
    "\n",
    "**Research question.**  \n",
    "Does **NAcc lateralization** (left minus right) during anticipation change from age 16 to 21?\n",
    "\n",
    "- **H₀:** The mean change in `NAcc_lat` from 16 to 21 is 0.  \n",
    "- **H₁:** The mean change in `NAcc_lat` from 16 to 21 is **not** 0.\n",
    "\n",
    "---\n",
    "\n",
    "### What to report for Your Turn 1\n",
    "\n",
    "For the **one option** you choose, report:\n",
    "\n",
    "- Mean of your measure at **age 16** and **age 21**.\n",
    "- Mean change (**21 − 16**).\n",
    "- Paired t-test result: *t*, df, and *p*-value.\n",
    "- 95% confidence interval for the mean change.\n",
    "- Cohen’s *d* (paired).\n",
    "- A short interpretation in words (direction, size, and practical meaning).\n",
    "\n",
    "Include **two figures**:\n",
    "\n",
    "1. **Histogram of within-subject change scores (21 − 16)** — used as a *diagnostic* to check the normality assumption for the paired t-test.  \n",
    "2. **Main result figure** — one plot that clearly shows the age effect (we recommend a simple bar chart with mean ± standard error for age 16 vs 21).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1fcd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Your Turn 1: Analysis Steps\n",
    "# (Fill in code under each step)\n",
    "# ===========================\n",
    "\n",
    "# 0. Make sure you have:\n",
    "# - df loaded (long format, with 'subject_id' and 'wave' = 16 or 21)\n",
    "# - Your chosen measure or derived variable computed at each wave\n",
    "#   (e.g., bias_ventral_vs_dorsal, bias_nacc_vs_put, NAcc_ant_out_gap, NAcc_lat)\n",
    "\n",
    "# 1. PIVOT TO WIDE FORMAT\n",
    "# -----------------------\n",
    "# Goal: create a wide DataFrame with one row per subject and\n",
    "#       two columns: one for age 16, one for age 21.\n",
    "#\n",
    "# Example pattern (replace YOUR_VARIABLE_NAME with your chosen measure):\n",
    "#   wide = (\n",
    "#       df.pivot(index='subject_id', columns='wave', values='YOUR_VARIABLE_NAME')\n",
    "#         .dropna()\n",
    "#   )\n",
    "#   wide.columns = ['val_16', 'val_21']\n",
    "\n",
    "# TODO: write your pivot code here:\n",
    "# wide = ...\n",
    "# wide.columns = ['val_16', 'val_21']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce894c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DESCRIPTIVE STATISTICS\n",
    "# -------------------------\n",
    "# Compute:\n",
    "# - mean at 16\n",
    "# - mean at 21\n",
    "# - within-subject change scores (21 − 16)\n",
    "# - SD of the change scores\n",
    "# - sample size n\n",
    "\n",
    "# TODO: compute diff = wide['val_21'] - wide['val_16']\n",
    "# diff = ...\n",
    "\n",
    "# TODO: compute mean16, mean21, mean_diff, sd_diff, n\n",
    "# mean16 = ...\n",
    "# mean21 = ...\n",
    "# mean_diff = ...\n",
    "# sd_diff = ...\n",
    "# n = ...\n",
    "\n",
    "# TODO: print descriptive results in a clear way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PAIRED T-TEST\n",
    "# ----------------\n",
    "# Use scipy.stats.ttest_rel to compare val_21 vs val_16.\n",
    "\n",
    "# from scipy import stats  # (already imported earlier in the notebook)\n",
    "# TODO: run the paired t-test\n",
    "# t_stat, p_val = stats.ttest_rel(wide['val_21'], wide['val_16'])\n",
    "\n",
    "# TODO: print t-statistic, df (n-1), and p-value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 95% CONFIDENCE INTERVAL FOR MEAN CHANGE\n",
    "# ------------------------------------------\n",
    "# Use the t distribution:\n",
    "#   se_diff = sd_diff / np.sqrt(n)\n",
    "#   ci_low, ci_high = stats.t.interval(0.95, df=n-1, loc=mean_diff, scale=se_diff)\n",
    "\n",
    "# TODO: compute se_diff, ci_low, ci_high\n",
    "# se_diff = ...\n",
    "# ci_low, ci_high = ...\n",
    "\n",
    "# TODO: print the CI nicely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. COHEN'S d (PAIRED)\n",
    "# ---------------------\n",
    "# For paired data:\n",
    "#   d = mean_diff / sd_diff\n",
    "\n",
    "# TODO: compute cohen_d and print it\n",
    "# cohen_d = ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5162ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. PLOT 1: HISTOGRAM OF CHANGE SCORES\n",
    "# -------------------------------------\n",
    "# This is the diagnostic plot to check whether diff is roughly symmetric/normal.\n",
    "\n",
    "# import matplotlib.pyplot as plt  # already imported earlier\n",
    "# TODO: make a histogram of diff with a vertical line at 0\n",
    "# plt.figure()\n",
    "# plt.hist(..., bins=..., edgecolor='black')\n",
    "# plt.axvline(0, color='k', linestyle='--')\n",
    "# plt.xlabel('Change score (21 − 16)')\n",
    "# plt.ylabel('Number of participants')\n",
    "# plt.title('Distribution of within-subject change scores')\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ba77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. PLOT 2: MAIN RESULT FIGURE (BAR PLOT)\n",
    "# ----------------------------------------\n",
    "# This is the main visualization to summarize the effect:\n",
    "# - x-axis: Age 16 vs Age 21\n",
    "# - y-axis: mean of your measure\n",
    "# - error bars: standard error (SD / sqrt(n))\n",
    "\n",
    "# TODO: compute standard errors for each age\n",
    "# se16 = ...\n",
    "# se21 = ...\n",
    "\n",
    "# means = [mean16, mean21]\n",
    "# ses = [se16, se21]\n",
    "\n",
    "# TODO: make a bar plot with error bars (use capsize for visible error bars)\n",
    "# plt.figure()\n",
    "# plt.bar(['Age 16', 'Age 21'], means, yerr=ses, capsize=5)\n",
    "# plt.axhline(0, color='k', linestyle='--')  # optional reference line at 0\n",
    "# plt.ylabel('Your measure (specify units)')\n",
    "# plt.title('Change in your chosen measure from 16 to 21')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f076a6",
   "metadata": {},
   "source": [
    "## Activity 2: Independent Samples t-Tests (Group Differences in NAcc Activation)\n",
    "\n",
    "Activity 2 shifts to **between-subject comparisons**. We will test whether brain activation at age 21 differs between groups defined by some behavioral or individual difference variables. For example, do individuals with high vs low levels of a behavior show different NAcc activation? Each analysis will involve:\n",
    "\n",
    "- Checking assumptions (normal-ish distribution, and whether variances seem unequal – we will use Welch’s t-test which does not assume equal variances).  \n",
    "- Performing the two-sample t-test.  \n",
    "- Computing Cohen’s d for the group difference.  \n",
    "- Computing a 95% CI for the difference.  \n",
    "- Plotting the group distributions for visualization.  \n",
    "- Interpreting both statistical and practical significance.\n",
    "\n",
    "We will fix the outcome variable as the bilateral NAcc reward anticipation beta at age 21 for all tests (since that’s a focal brain measure). We’ll try different grouping variables.  \n",
    "**The bilateral NAcc reward anticipation beta reflects how strongly the nucleus accumbens responds during the cue period when a reward *might* be earned, capturing anticipatory activation in this key reward region.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944536ad",
   "metadata": {},
   "source": [
    "### Demo 2: High vs Low Social Media Use – NAcc Reward Anticipation\n",
    "\n",
    "Our demo compares two groups defined by social media use at age 21: those who use it a lot vs those who use it very little. The hypothesis (if any) could be exploratory – perhaps heavy social media users might have blunted reward system responses (due to overstimulation), or maybe more active reward systems (if seeking rewards). We’ll test it empirically.\n",
    "\n",
    "**Grouping variable:** We have a variable `nt_y_stq__socmed__hr_001` which indicates hours of social media use per day. We’ll create “Low use” and “High use” groups. To clearly separate them, we take the bottom quartile as Low and top quartile as High:\n",
    "\n",
    "- Low social media use = bottom 25% of users (approximately ≤ 1 hour/day).\n",
    "- High social media use = top 25% (approximately ≥ 4.5 hours/day).\n",
    "\n",
    "This gives two groups with N ~ 2,756 each (since ~11k total). We ensure no overlap (we drop the middle 50% for a cleaner comparison).\n",
    "\n",
    "**Workflow:**\n",
    "\n",
    "1. **Define groups and outcome:** Select participants in the low and high quartiles of social media hours, and extract their NAcc anticipation beta at age 21.\n",
    "2. **Descriptive statistics:** Compute the mean, SD, and n for each group (Low vs High) and the mean difference (High − Low).\n",
    "3. **Plot 1 – Assumption check:** Make histograms (or density plots) of the NAcc anticipation beta for each group and compare their spreads. This helps you see whether distributions look roughly normal and whether variances are similar or different.\n",
    "4. **Two-sample t-test:** Use **Welch’s t-test** (unequal variances) to test whether the group means differ.\n",
    "5. **95% confidence interval:** Compute a 95% CI for the mean difference (High − Low).\n",
    "6. **Effect size:** Compute **Cohen’s d** for the group difference.\n",
    "7. **Plot 2 – Main result:** Make one main figure that clearly shows the group difference (for example, a bar plot of the two group means with standard error bars, or a side-by-side boxplot).\n",
    "8. **Interpretation:** Write a short interpretation that ties together direction, size, statistical significance, and practical significance of the group difference.\n",
    "\n",
    "> **Why Welch’s t-test?**  \n",
    "> In your Intro Stat text, the two-sample *t*-test is usually shown with a formula that assumes the two groups have **equal variances**. In real data (and especially with very large samples), group variances often differ a bit. **Welch’s t-test** is a slightly more flexible version of the same two-sample *t*-test that *does not* assume equal variances. The logic and interpretation are the same as the two-sample *t*-test you’ve seen (difference in means, test statistic, p-value, CI); it just uses a slightly different standard error and degrees of freedom formula to be safer when group spreads are unequal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac97509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: High vs Low Social Media Use – NAcc Reward Anticipation\n",
    "# (Assumes df, np, stats, plt, and math have already been imported.)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Define groups and outcome (age 21 only)\n",
    "# ---------------------------------------------------------\n",
    "wave21_data = df[df['wave'] == 21].copy()\n",
    "wave21_data = wave21_data[['subject_id', 'NAcc_ant_bilateral', 'nt_y_stq__socmed__hr_001']].dropna()\n",
    "\n",
    "social_hours = wave21_data['nt_y_stq__socmed__hr_001']\n",
    "q1 = social_hours.quantile(0.25)\n",
    "q3 = social_hours.quantile(0.75)\n",
    "\n",
    "low_mask = social_hours <= q1\n",
    "high_mask = social_hours >= q3\n",
    "\n",
    "low_group = wave21_data[low_mask]['NAcc_ant_bilateral']\n",
    "high_group = wave21_data[high_mask]['NAcc_ant_bilateral']\n",
    "\n",
    "n_low, n_high = len(low_group), len(high_group)\n",
    "print(f\"N low = {n_low}, N high = {n_high}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Descriptive statistics\n",
    "# ---------------------------------------------------------\n",
    "mean_low, mean_high = low_group.mean(), high_group.mean()\n",
    "sd_low, sd_high = low_group.std(), high_group.std()\n",
    "\n",
    "diff = mean_high - mean_low  # High − Low\n",
    "print(f\"Low SM mean = {mean_low:.3f} (SD = {sd_low:.3f})\")\n",
    "print(f\"High SM mean = {mean_high:.3f} (SD = {sd_high:.3f})\")\n",
    "print(f\"Mean difference (High − Low) = {diff:.3f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Plot 1 – Assumption check (histograms of each group)\n",
    "# ---------------------------------------------------------\n",
    "plt.figure()\n",
    "plt.hist(low_group, bins=30, alpha=0.5, density=True, label='Low SM')\n",
    "plt.hist(high_group, bins=30, alpha=0.5, density=True, label='High SM')\n",
    "plt.axvline(mean_low, color='blue', linestyle='--')\n",
    "plt.axvline(mean_high, color='orange', linestyle='--')\n",
    "plt.xlabel('NAcc anticipation beta @21')\n",
    "plt.ylabel('Density')\n",
    "plt.title('High vs Low Social Media: NAcc activation (assumption check)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Two-sample test – Welch’s t-test (unequal variances)\n",
    "# ---------------------------------------------------------\n",
    "t_stat, p_val = stats.ttest_ind(high_group, low_group, equal_var=False)\n",
    "\n",
    "# Welch degrees of freedom\n",
    "var_low, var_high = sd_low**2, sd_high**2\n",
    "df_welch = (var_low/n_low + var_high/n_high)**2 / (\n",
    "    (var_low**2 / ((n_low - 1) * n_low**2)) +\n",
    "    (var_high**2 / ((n_high - 1) * n_high**2))\n",
    ")\n",
    "\n",
    "print(f\"Welch t-test: t = {t_stat:.2f}, p = {p_val:.3g}, df ≈ {df_welch:.1f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. 95% confidence interval for the mean difference\n",
    "# ---------------------------------------------------------\n",
    "se_diff = math.sqrt(var_low/n_low + var_high/n_high)\n",
    "ci_lower, ci_upper = stats.t.interval(\n",
    "    0.95, df=df_welch, loc=diff, scale=se_diff\n",
    ")\n",
    "\n",
    "print(f\"95% CI for mean difference (High − Low): [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. Effect size – Cohen’s d for group difference\n",
    "#    (using pooled SD of the two groups)\n",
    "# ---------------------------------------------------------\n",
    "pooled_sd = math.sqrt(((n_low - 1)*var_low + (n_high - 1)*var_high) / (n_low + n_high - 2))\n",
    "cohen_d = diff / pooled_sd\n",
    "print(f\"Cohen's d = {cohen_d:.2f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. Plot 2 – Main result (bar plot with SE error bars)\n",
    "# ---------------------------------------------------------\n",
    "se_low = sd_low / math.sqrt(n_low)\n",
    "se_high = sd_high / math.sqrt(n_high)\n",
    "\n",
    "means = [mean_low, mean_high]\n",
    "ses = [se_low, se_high]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(['Low SM', 'High SM'], means, yerr=ses, capsize=5)\n",
    "plt.axhline(0, color='k', linestyle='--', linewidth=0.8)\n",
    "plt.ylabel('NAcc anticipation beta @21')\n",
    "plt.title('High vs Low Social Media: NAcc activation (main result)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649d400",
   "metadata": {},
   "source": [
    "**Interpretation (Demo 2).**  \n",
    "In this sample, high social media users had slightly higher NAcc reward anticipation beta on average (High SM mean ≈ 0.064 vs Low SM mean ≈ 0.054), but the difference was very small (mean difference ≈ 0.010). Welch’s *t*-test was not statistically significant (*t* ≈ 1.25, *p* ≈ 0.21), and the 95% confidence interval for the mean difference ([-0.006, 0.026]) includes zero. The effect size is **tiny** (Cohen’s *d* ≈ 0.04), indicating a practically negligible difference between the groups. The overlapping histograms and similar bar-plot means reinforce this: in practical terms, adolescents and young adults with very high vs very low social media use show **no meaningful difference** in NAcc reward anticipation activity at age 21 in this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d36d6",
   "metadata": {},
   "source": [
    "### Your Turn 2: Other Independent t-Test Analyses (Choose ONE option)\n",
    "\n",
    "In Demo 2 we tested **between-subject group differences** in bilateral NAcc activation at age 21 using Welch’s two-sample *t*-test. Now it’s your turn to run a similar independent *t*-test on a different grouping variable.\n",
    "\n",
    "For **all options**, the **outcome variable** is:\n",
    "\n",
    "- **Bilateral NAcc reward anticipation beta at age 21**  \n",
    "  (`NAcc_ant_bilateral` for participants in wave 21)\n",
    "\n",
    "Pick **ONE** of the options (A–D) below. For your chosen option you will define two groups, run an independent-samples *t*-test, and interpret the result.\n",
    "\n",
    "**Options (grouping variables and questions):**\n",
    "\n",
    "- **Option A – Cannabis use (past 6 months)**  \n",
    "  - Variable: `su_y_mysu__use__mj__6mo_001` (0 = no use, 1 = any use, 777 = missing).  \n",
    "  - Question: Do participants who used cannabis in the past 6 months differ from non-users in NAcc reward anticipation activation at age 21?  \n",
    "  - Hypothesis (exploratory): chronic cannabis use might be associated with slightly **blunted** reward response (users < non-users).\n",
    "\n",
    "- **Option B – Alcohol use (past 6 months)**  \n",
    "  - Variable: `su_y_mysu__use__alc__6mo_001` (0 = no use, 1 = any use, 777 = missing).  \n",
    "  - Question: Do alcohol users differ from abstainers in NAcc reward anticipation activation at age 21?  \n",
    "  - Hypothesis: repeated alcohol use could dampen natural reward responses (drinkers < non-drinkers).\n",
    "\n",
    "- **Option C – Externalizing genetic risk (polygenic score)**  \n",
    "  - Variable: `pgs_externalizing_std` (standardized polygenic score).  \n",
    "  - Question: Does NAcc reward anticipation activation differ between people with **high** vs **low** genetic risk for externalizing behavior?  \n",
    "  - Suggestion: define High risk as the **top ~20%** of the score distribution and Low risk as the **bottom ~20%**.  \n",
    "  - Hypothesis (exploratory): high-risk individuals might show slightly different reward sensitivity (either higher or lower).\n",
    "\n",
    "- **Option D – Sex differences**  \n",
    "  - Variable: `ab_g_stc__cohort_sex` (e.g., `\"M\"` vs `\"F\"`).  \n",
    "  - Question: Do males and females differ in NAcc reward anticipation activation at age 21?  \n",
    "  - Hypothesis: prior findings are mixed; we simply test for **any** mean difference.\n",
    "\n",
    "*Note:* For binary substance-use variables, restrict to codes **0 or 1** and drop missing/skip codes like **777**.\n",
    "\n",
    "---\n",
    "\n",
    "### Workflow for Your Turn 2 (for the ONE option you choose)\n",
    "\n",
    "1. **Define groups and outcome:**  \n",
    "   Subset to wave 21, handle missing codes, and create two groups (Group 1 vs Group 2) based on your chosen variable. Extract `NAcc_ant_bilateral` for each group.\n",
    "\n",
    "2. **Descriptive statistics:**  \n",
    "   Compute the mean, SD, and *n* for each group, and the mean difference (Group 2 − Group 1). Clearly label which group is which.\n",
    "\n",
    "3. **Plot 1 – Assumption check:**  \n",
    "   Make histograms (or density plots) of `NAcc_ant_bilateral` for each group. Use this to check whether distributions look roughly normal and whether group spreads (variances) are similar or different.\n",
    "\n",
    "4. **Two-sample t-test (Welch):**  \n",
    "   Run Welch’s independent-samples *t*-test using  \n",
    "   `stats.ttest_ind(group2, group1, equal_var=False)` to test for any difference in means.\n",
    "\n",
    "5. **95% confidence interval:**  \n",
    "   Compute a 95% CI for the mean difference (Group 2 − Group 1).\n",
    "\n",
    "6. **Effect size:**  \n",
    "   Compute **Cohen’s *d*** for the group difference.\n",
    "\n",
    "7. **Plot 2 – Main result:**  \n",
    "   Make one main figure that clearly shows the group difference (for example, a bar plot of the two group means with **standard error** bars, or a side-by-side boxplot).\n",
    "\n",
    "8. **Interpretation:**  \n",
    "   Write 2–3 sentences interpreting the result (direction of the difference, effect size, statistical significance, and practical meaning).\n",
    "\n",
    "---\n",
    "\n",
    "### Visualizations required (2 total)\n",
    "\n",
    "- **Plot 1 (assumption check):** histogram/density of `NAcc_ant_bilateral` for each group — used as a **diagnostic** to check assumptions for the *t*-test.  \n",
    "- **Plot 2 (main result):** bar plot with error bars (or boxplot) for Group 1 vs Group 2 — used as the **main figure** to communicate your result (direction, size, and uncertainty).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e573dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Your Turn 2: Analysis Steps\n",
    "# (Fill in code under each step)\n",
    "# ===========================\n",
    "\n",
    "# 0. Make sure you have:\n",
    "# - df loaded with wave == 21 included\n",
    "# - Outcome variable: NAcc_ant_bilateral at age 21\n",
    "# - ONE grouping variable chosen (Options A–D):\n",
    "#     A: su_y_mysu__use__mj__6mo_001       (0 = no use, 1 = any use, 777 = missing)\n",
    "#     B: su_y_mysu__use__alc__6mo_001      (0 = no use, 1 = any use, 777 = missing)\n",
    "#     C: pgs_externalizing_std             (continuous polygenic score)\n",
    "#     D: ab_g_stc__cohort_sex              (e.g., \"M\" vs \"F\")\n",
    "#\n",
    "# You will compare TWO groups on NAcc_ant_bilateral @21 using Welch’s t-test.\n",
    "\n",
    "# 1. DEFINE GROUPS AND OUTCOME\n",
    "# ----------------------------\n",
    "# Goal: subset to wave 21, handle missing codes, and create:\n",
    "#   - group1: NAcc_ant_bilateral values for Group 1\n",
    "#   - group2: NAcc_ant_bilateral values for Group 2\n",
    "#\n",
    "# Example pattern (you MUST edit this to match your option):\n",
    "#\n",
    "#   wave21 = df[df['wave'] == 21].copy()\n",
    "#   group_var_name = 'su_y_mysu__use__mj__6mo_001'   # TODO: change for Options B–D\n",
    "#\n",
    "#   # For 0/1 variables, keep only valid codes\n",
    "#   wave21 = wave21[wave21[group_var_name].isin([0.0, 1.0])]\n",
    "#\n",
    "#   group1_mask = wave21[group_var_name] == 0.0      # e.g., Non-users\n",
    "#   group2_mask = wave21[group_var_name] == 1.0      # e.g., Users\n",
    "#\n",
    "#   group1_label = 'Non-users'\n",
    "#   group2_label = 'Users'\n",
    "#\n",
    "#   group1 = wave21.loc[group1_mask, 'NAcc_ant_bilateral']\n",
    "#   group2 = wave21.loc[group2_mask, 'NAcc_ant_bilateral']\n",
    "#\n",
    "# TODO: write your own grouping code here (pick ONE option and define group1/group2):\n",
    "\n",
    "# wave21 = ...\n",
    "# group_var_name = ...\n",
    "# (filter missing / invalid codes if needed)\n",
    "# group1_mask = ...\n",
    "# group2_mask = ...\n",
    "# group1_label = ...\n",
    "# group2_label = ...\n",
    "# group1 = ...\n",
    "# group2 = ...\n",
    "\n",
    "# After you define the groups, compute sample sizes and print them:\n",
    "# n1 = len(group1)\n",
    "# n2 = len(group2)\n",
    "# print(f\"{group1_label}: n = {n1}\")\n",
    "# print(f\"{group2_label}: n = {n2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b32cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DESCRIPTIVE STATISTICS\n",
    "# -------------------------\n",
    "# Compute:\n",
    "# - mean and SD for each group\n",
    "# - mean difference (Group2 − Group1)\n",
    "\n",
    "# TODO: compute mean1, mean2, sd1, sd2, diff\n",
    "# mean1 = ...\n",
    "# mean2 = ...\n",
    "# sd1 = ...\n",
    "# sd2 = ...\n",
    "# diff = ...   # mean2 - mean1\n",
    "\n",
    "# TODO: print the descriptive results clearly\n",
    "# print(f\"{group1_label} mean = {mean1:.3f} (SD = {sd1:.3f})\")\n",
    "# print(f\"{group2_label} mean = {mean2:.3f} (SD = {sd2:.3f})\")\n",
    "# print(f\"Mean difference ({group2_label} − {group1_label}) = {diff:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79722126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PLOT 1: ASSUMPTION CHECK (HISTOGRAMS)\n",
    "# ----------------------------------------\n",
    "# Make histograms (or density plots) of NAcc_ant_bilateral for each group.\n",
    "# This is to visually check normal-ish shape and compare spreads.\n",
    "\n",
    "# import matplotlib.pyplot as plt  # already imported earlier\n",
    "# TODO: make overlapping histograms with vertical lines at means\n",
    "# plt.figure()\n",
    "# plt.hist(..., bins=30, alpha=0.5, density=True, label=group1_label)\n",
    "# plt.hist(..., bins=30, alpha=0.5, density=True, label=group2_label)\n",
    "# plt.axvline(mean1, color='blue', linestyle='--')\n",
    "# plt.axvline(mean2, color='orange', linestyle='--')\n",
    "# plt.xlabel('NAcc anticipation beta @21')\n",
    "# plt.ylabel('Density')\n",
    "# plt.title(f'{group1_label} vs {group2_label}: NAcc activation (assumption check)')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TWO-SAMPLE T-TEST (WELCH)\n",
    "# ----------------------------\n",
    "# Use scipy.stats.ttest_ind with equal_var=False.\n",
    "# This compares the group means without assuming equal variances.\n",
    "#\n",
    "# from scipy import stats  # already imported earlier\n",
    "\n",
    "# TODO: run Welch's t-test\n",
    "# t_stat, p_val = stats.ttest_ind(group2, group1, equal_var=False)\n",
    "\n",
    "# Compute Welch degrees of freedom:\n",
    "# v1 = (sd1**2) / n1\n",
    "# v2 = (sd2**2) / n2\n",
    "# df_welch = (v1 + v2)**2 / ((v1**2 / (n1 - 1)) + (v2**2 / (n2 - 1)))\n",
    "\n",
    "# TODO: print t-statistic, df_welch, and p-value\n",
    "# print(f\"Welch t-test: t = {t_stat:.2f}, p = {p_val:.3g}, df ≈ {df_welch:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878aaa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 95% CONFIDENCE INTERVAL FOR MEAN DIFFERENCE\n",
    "# ----------------------------------------------\n",
    "# Use the t distribution to build a CI for (Group2 − Group1):\n",
    "#\n",
    "#   se_diff = sqrt( (sd1^2 / n1) + (sd2^2 / n2) )\n",
    "#   ci_low, ci_high = stats.t.interval(\n",
    "#       0.95, df=df_welch, loc=diff, scale=se_diff\n",
    "#   )\n",
    "\n",
    "# import math  # already imported earlier\n",
    "# TODO: compute se_diff, ci_low, ci_high\n",
    "# se_diff = ...\n",
    "# ci_low, ci_high = ...\n",
    "\n",
    "# TODO: print the CI nicely\n",
    "# print(f\"95% CI for mean difference ({group2_label} − {group1_label}): \"\n",
    "#       f\"[{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3767e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. COHEN'S d (INDEPENDENT GROUPS)\n",
    "# ---------------------------------\n",
    "# Effect size for group difference using pooled SD:\n",
    "#\n",
    "#   pooled_sd = sqrt( ((n1 - 1)*sd1^2 + (n2 - 1)*sd2^2) / (n1 + n2 - 2) )\n",
    "#   cohen_d = diff / pooled_sd\n",
    "\n",
    "# TODO: compute pooled_sd and cohen_d, then print\n",
    "# pooled_sd = ...\n",
    "# cohen_d = ...\n",
    "# print(f\"Cohen's d = {cohen_d:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d7bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. PLOT 2: MAIN RESULT FIGURE (BAR PLOT)\n",
    "# ----------------------------------------\n",
    "# Main visualization to summarize the effect:\n",
    "# - x-axis: Group 1 vs Group 2\n",
    "# - y-axis: mean NAcc_ant_bilateral\n",
    "# - error bars: standard error for each group (SD / sqrt(n))\n",
    "\n",
    "# TODO: compute standard errors\n",
    "# se1 = ...\n",
    "# se2 = ...\n",
    "\n",
    "# means = [mean1, mean2]\n",
    "# ses = [se1, se2]\n",
    "\n",
    "# TODO: make a bar plot with error bars\n",
    "# plt.figure()\n",
    "# plt.bar([group1_label, group2_label], means, yerr=ses, capsize=5)\n",
    "# plt.axhline(0, color='k', linestyle='--', linewidth=0.8)  # optional reference line\n",
    "# plt.ylabel('NAcc anticipation beta @21')\n",
    "# plt.title(f'{group1_label} vs {group2_label}: NAcc activation (main result)')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70014a86",
   "metadata": {},
   "source": [
    "## Activity 3: Power and Sample Size Analysis\n",
    "\n",
    "In Activities 1 and 2 we ran a bunch of hypothesis tests on NAcc activation:\n",
    "\n",
    "- **Activity 1:** paired *t*-tests for **within-subject change from age 16 to 21** (longitudinal NAcc betas).\n",
    "- **Activity 2:** independent-samples *t*-tests for **group differences at age 21** (social media, substance use, PGS, sex, etc.).\n",
    "\n",
    "We often found **very small effect sizes** (Cohen’s *d* ≈ 0.01–0.04), sometimes with “significant” *p*-values when the sample size was huge. In this activity we step back and ask:\n",
    "\n",
    "- How likely are we to see a **false positive** (*p* < 0.05) when there is **no real effect**?\n",
    "- Given an effect size like the ones you estimated in Activities 1 or 2, **what is the power** of our tests at different sample sizes?\n",
    "- How many participants would we need to design a new study with, say, **80% power** to detect effects of different sizes?\n",
    "\n",
    "We will use simulations and analytic calculations to explore these questions and connect them back to your own Activity 1 and 2 results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4707fdd1",
   "metadata": {},
   "source": [
    "### Task 1: Simulating Type I Error (False Positives)\n",
    "\n",
    "A good scientist knows that if there is truly **no difference** between two groups, about 5% of independent tests will, just by chance, yield *p* < 0.05 (a **false positive**) when we use α = 0.05.\n",
    "\n",
    "In **Activity 2**, you may have run **several different group comparisons** (social media, cannabis, alcohol, PGS, sex). If all of these truly had **no effect** on NAcc activation, each test would still have a 5% chance of “lighting up” with *p* < 0.05 just by luck. With multiple tests, the chance that **at least one** is “significant” gets even higher.\n",
    "\n",
    "In this task, we simulate that situation directly: we generate two groups from the **same distribution** (no real effect), run a huge number of independent *t*-tests, and see what fraction come out as false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d44bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "n_per_group = 100\n",
    "num_sim = 10000\n",
    "alpha = 0.05\n",
    "false_positives = 0\n",
    "\n",
    "for i in range(num_sim):\n",
    "    # Generate data under null: both groups ~ N(0,1)\n",
    "    group1 = np.random.randn(n_per_group)\n",
    "    group2 = np.random.randn(n_per_group)\n",
    "    _, p = stats.ttest_ind(group1, group2, equal_var=True)  # both groups same variance from same dist\n",
    "    if p < alpha:\n",
    "        false_positives += 1\n",
    "\n",
    "fp_rate = false_positives / num_sim\n",
    "print(f\"Simulated Type I error rate = {fp_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724fea6",
   "metadata": {},
   "source": [
    "**Connection to Activity 2.**  \n",
    "Compare the simulated ~5% false-positive rate to how many “significant” group effects you saw in Activity 2. If you ran 4–5 tests, it would not be surprising to see **one** *p* < 0.05 purely by chance. This helps you interpret tiny but “significant” results (especially when *d* is very close to 0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643fe924",
   "metadata": {},
   "source": [
    "### Task 2: Simulating Power for *Your* Effect Size\n",
    "\n",
    "Power is the probability of correctly rejecting the null when a true effect exists. It depends on three things:\n",
    "effect size (*d*), sample size (*n* per group), and α.\n",
    "\n",
    "In this task, you will connect power to **your own results**:\n",
    "\n",
    "1. Choose **one Cohen’s d** from Activity 1 or Activity 2  \n",
    "   - Example: the *d* for NAcc change from 16 → 21 (Activity 1), or  \n",
    "   - The *d* for one of your group comparisons (cannabis, alcohol, PGS, sex) from Activity 2.\n",
    "2. Treat that *d* as the **true effect size in the population**.\n",
    "3. Simulate many “experiments” with a realistic fMRI sample size (e.g., 30, 40, 60 participants per group) and\n",
    "   estimate **power** (the fraction of simulations with *p* < 0.05).\n",
    "4. Then repeat with a more “textbook” effect size (e.g., *d* = 0.3) so you can compare.\n",
    "\n",
    "You will use the code below by filling in:\n",
    "- `d_true` with your chosen Cohen’s *d*  \n",
    "- `n_per_group` with a sample size you want to explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Task 2: Simulating power for your effect size\n",
    "# (Fill in d_true and n_per_group)\n",
    "# ============================================\n",
    "\n",
    "# 1. CHOOSE AN EFFECT SIZE\n",
    "# ------------------------\n",
    "# Option 1: Use a value from Activities 1 or 2\n",
    "#   e.g., d_true = 0.03\n",
    "# Option 2: Use a target effect size\n",
    "#   e.g., d_true = 0.3\n",
    "\n",
    "d_true = __    # TODO: replace with your own Cohen's d\n",
    "\n",
    "# 2. CHOOSE A SAMPLE SIZE PER GROUP\n",
    "# ---------------------------------\n",
    "# Try values like 30, 40, 60, 100 and see how power changes.\n",
    "\n",
    "n_per_group = __   # TODO: set a sample size per group\n",
    "\n",
    "# 3. RUN THE POWER SIMULATION\n",
    "# ---------------------------\n",
    "\n",
    "np.random.seed(0)\n",
    "num_sim = 5000\n",
    "alpha = 0.05\n",
    "detections = 0\n",
    "\n",
    "n_per_group = int(n_per_group)\n",
    "\n",
    "for i in range(num_sim):\n",
    "    # Group 1 ~ N(0, 1)\n",
    "    group1 = np.random.randn(n_per_group)\n",
    "    # Group 2 ~ N(d_true, 1)  (shifted by d_true SD units)\n",
    "    group2 = np.random.randn(n_per_group) + d_true\n",
    "\n",
    "    _, p = stats.ttest_ind(group2, group1, equal_var=False)\n",
    "    if p < alpha:\n",
    "        detections += 1\n",
    "\n",
    "power_est = detections / num_sim\n",
    "print(f\"Estimated power for d = {d_true} with n = {n_per_group} per group is {power_est:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d93b07a",
   "metadata": {},
   "source": [
    "**What to notice**\n",
    "\n",
    "- If you plug in your tiny effect from Activity 1 or 2 (often *d* ≈ 0.01–0.04) and a small sample\n",
    "  (e.g., 30–50 per group), the estimated power should be **very close to 0**. A typical lab study would almost\n",
    "  never detect such an effect.\n",
    "- If you plug in a more realistic effect (e.g., *d* = 0.3) and try different `n_per_group` values, watch how\n",
    "  power increases. For example, you might see something like:\n",
    "  \n",
    "  > Estimated power for d = 0.3 with n = 50 per group is 0.50\n",
    "  \n",
    "  which means only about half of such studies would find a significant result.\n",
    "- You will use these observations in the written reflection to explain why large cohorts (like ABCD) can detect\n",
    "  **tiny** effects, and why small, underpowered studies often miss even modest ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362034a",
   "metadata": {},
   "source": [
    "### Task 3: Required Sample Size for Specific Effect Sizes\n",
    "\n",
    "So far we’ve looked at standard textbook effect sizes: *d* = 0.2, 0.3, 0.5. Now connect this back to your own results:\n",
    "\n",
    "1. Use the code below to compute required *n* per group for *d* = 0.2, 0.3, 0.5 with 80% power.\n",
    "2. **Extension (recommended):** Plug in the **Cohen’s *d*** from one of your Activity 2 group comparisons and see how big *n* per group would need to be for 80% power.\n",
    "\n",
    "If your observed *d* was ≈ 0.02 or 0.03, you’ll see that the required sample size is enormous (many thousands) — essentially, only a very large cohort like this synthetic ABCD sample could detect it reliably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca7a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "effect_sizes = [0.2, 0.3, 0.5]\n",
    "\n",
    "# OPTIONAL: add your own effect size from Activities 1 or 2\n",
    "# my_d = 0.03   # TODO: replace with your Cohen's d (if you want to explore it)\n",
    "# effect_sizes.append(my_d)\n",
    "\n",
    "analysis = TTestIndPower()\n",
    "for d in effect_sizes:   # <-- use effect_sizes here\n",
    "    n = analysis.solve_power(effect_size=d, power=0.8, alpha=0.05,\n",
    "                             ratio=1, alternative='two-sided')\n",
    "    print(f\"d={d:.1f} requires ~{math.ceil(n)} per group (total ~{math.ceil(n)*2})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24caacb",
   "metadata": {},
   "source": [
    "### Big-picture wrap-up\n",
    "\n",
    "Across these three activities, you have now seen how **effect size, sample size, and power** fit together.\n",
    "\n",
    "- **Activities 1 & 2:** You estimated real effect sizes (often tiny, *d* ≈ 0.01–0.04) and checked whether the corresponding *p*-values were significant.\n",
    "- **Activity 3:** You explored how often tests give false positives, how power changes with *n* and *d*, and how large samples must be to reach 80% power.\n",
    "\n",
    "From the sample-size calculations (for 80% power, α = 0.05), we saw that:\n",
    "\n",
    "- *d* = 0.2 → ~394 per group (total ~788)  \n",
    "- *d* = 0.3 → ~176 per group (total ~352)  \n",
    "- *d* = 0.5 → ~64 per group (total ~128)\n",
    "\n",
    "**Take-home messages:**\n",
    "\n",
    "- Small effects (like *d* = 0.2) require very large samples to detect reliably.  \n",
    "- Medium effects (*d* ~ 0.5) can be detected with more typical lab sizes (tens of people per group).  \n",
    "- The effects we observed in Activities 1–2 (*d* ≈ 0.01–0.04) are so small that only a very large cohort (like this synthetic ABCD sample) has enough power to see them.\n",
    "\n",
    "So, when planning a new study:\n",
    "\n",
    "- **Match your sample size to the effect size you actually care about.** If you can only recruit ~40 people per group, you cannot expect to reliably detect a *d* = 0.2 effect.  \n",
    "- **Avoid overinterpreting tiny but “significant” effects** that appear in huge samples; they may be statistically real but practically trivial.  \n",
    "- **Be cautious with multiple comparisons**: running many tests increases the chance of at least one false positive, so interpretation (and sometimes α) should be adjusted accordingly.\n",
    "\n",
    "Overall, the lab connects real fMRI analyses (Activities 1–2) with core ideas about power and sample size (Activity 3), giving you tools to both **interpret results** and **design better-powered studies** in the future.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
