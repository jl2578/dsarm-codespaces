{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67fffbb",
   "metadata": {},
   "source": [
    "# Lab 9: Investigating the Brain's \"Brake Cable\" Circuits with ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dadc7a",
   "metadata": {},
   "source": [
    "In this lab, we will explore the brain's \"brake system\" — the neural circuits responsible for inhibitory control — using our synthetic ABCD dataset. We will focus on the \"brake cables\" (white matter connections between the prefrontal cortex and subcortical regions) and examine how their integrity might differ across groups of adolescents with varying behaviors (like cannabis use). Our analysis will use ANOVA (Analysis of Variance) to test for group differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070909e",
   "metadata": {},
   "source": [
    "## Part 0: Setup – Data Loading and Preprocessing\n",
    "\n",
    "First, import the necessary libraries and load the dataset. Then we'll perform some cleaning (e.g. handle missing codes) and organize relevant variables.\n",
    "\n",
    "Data: We are using `labs/data/L1/ABCD_synthetic.csv`, a semi-synthetic dataset based on the Adolescent Brain Cognitive Development study. Each subject has multiple rows (different time points, labeled by `wave`). We'll treat each unique visit as a separate observation for analysis (later we may focus on a specific wave for cross-sectional analysis).\n",
    "\n",
    "Missing data codes: In this dataset, values like 777 or 999 indicate missing or non-applicable responses. We'll convert these to NaN (Not a Number) so that pandas treats them as missing values.\n",
    "\n",
    "Brake-circuit variables: We will define lists of variables corresponding to:\n",
    "- Functional brake cable measures (behavioral performance reflecting inhibitory control).\n",
    "- Structural brake cable measures (white matter integrity in key tracts like the uncinate fasciculus and anterior thalamic radiation).\n",
    "- Structural scaffolding measures (other white matter tracts that support the brake system, e.g. cingulum bundle, corpus callosum).\n",
    "\n",
    "Grouping variables: We will construct categorical grouping variables for:\n",
    "- Cannabis use severity – grouping participants by how much cannabis they've used.\n",
    "- Inhibitory control level – grouping by performance on an inhibitory control task (Flanker test accuracy).\n",
    "- (Optional) Adverse Childhood Experiences (ACE) score – grouping by total ACEs.\n",
    "- (Optional) Negative Urgency (impulsivity trait from UPPS questionnaire).\n",
    "- (Optional) Externalizing behaviors (from CBCL questionnaire).\n",
    "\n",
    "Let's execute these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe195a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Set plot style for clarity\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"labs/data/L1/ABCD_synthetic.csv\")\n",
    "\n",
    "# Replace missing codes with NaN\n",
    "df.replace([777, 999, 777.0, 999.0], np.nan, inplace=True)\n",
    "\n",
    "# Quick check: dimensions of the dataset\n",
    "print(\"Data shape:\", df.shape)\n",
    "# (We expect many rows since each subject can appear in multiple waves)\n",
    "\n",
    "# Define functional \"brake cable\" variables (e.g., behavioral measures of inhibitory control)\n",
    "functional_brake_vars = [\n",
    "    'nc_y_flnkr__incongr_acc'\n",
    "    # Flanker task incongruent accuracy (measure of inhibitory control)\n",
    "    # (Other functional measures of braking could be added here if available)\n",
    "]\n",
    "\n",
    "# Define structural \"brake cable\" variables (white matter tracts connecting PFC to subcortical regions)\n",
    "structural_brake_vars = [\n",
    "    'mr_y_dti__fs__fa__at__unc__lh_wmean',\n",
    "    # Left Uncinate Fasciculus FA (fractional anisotropy)\n",
    "    'mr_y_dti__fs__fa__at__unc__rh_wmean',\n",
    "    # Right Uncinate Fasciculus FA\n",
    "    'mr_y_dti__fs__fa__at__atr__lh_wmean',\n",
    "    # Left Anterior Thalamic Radiation FA\n",
    "    'mr_y_dti__fs__fa__at__atr__rh_wmean'\n",
    "    # Right Anterior Thalamic Radiation FA\n",
    "]\n",
    "\n",
    "# Define structural \"scaffolding\" variables (other structural connections supporting the brake system)\n",
    "structural_scaffold_vars = [\n",
    "    'mr_y_dti__fs__fa__at__fmin_wmean',\n",
    "    # Forceps minor (corpus callosum frontal fibers) FA\n",
    "    'mr_y_dti__fs__fa__at__cgc__lh_wmean',\n",
    "    # Left Cingulum (cingulate gyrus part) FA\n",
    "    'mr_y_dti__fs__fa__at__cgc__rh_wmean',\n",
    "    # Right Cingulum (cingulate) FA\n",
    "    'mr_y_dti__fs__fa__at__cgh__lh_wmean',\n",
    "    # Left Cingulum (hippocampal part) FA\n",
    "    'mr_y_dti__fs__fa__at__cgh__rh_wmean'\n",
    "    # Right Cingulum (hippocampal) FA\n",
    "]\n",
    "\n",
    "print(\"Functional brake vars:\", functional_brake_vars)\n",
    "print(\"Structural brake vars:\", structural_brake_vars)\n",
    "print(\"Structural scaffolding vars:\", structural_scaffold_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac02eee",
   "metadata": {},
   "source": [
    "Next, construct the grouping variables. These will be new columns in our DataFrame categorizing participants:\n",
    "\n",
    "- **Cannabis use severity** (`mj_group`): We will use two variables: `su_y_mysu__use__mj__6mo_001` (self-report of any marijuana use in the last 6 months) and `su_y_tlfb_mj_ud` (Timeline Followback: number of days of marijuana use in a recent period). We'll define severity as:\n",
    "  - 0 = Non-user (no use in the last 6 months),\n",
    "  - 1 = Moderate use (some use, but on fewer than 10 days),\n",
    "  - 2 = Heavy use (frequent use, e.g. 10 or more days of use).\n",
    "  \n",
    "  Note: The threshold of 10 days is arbitrary for this demonstration (in real analysis one might choose a different cutoff or use tertiles if appropriate). We combine information from both fields: if timeline data is missing but a youth reported trying cannabis, we'll classify them as moderate by default.\n",
    "\n",
    "- **Inhibitory control group** (`flanker_group`): Based on the Flanker incongruent accuracy (`nc_y_flnkr__incongr_acc`). We'll split into \"Low Inhibitory Control\" vs \"High Inhibitory Control\" groups, using the median accuracy as a cutoff. (High accuracy = better inhibitory control, Low = poorer control). Alternatively, one could use a preset performance threshold if available; here median split is a simple approach.\n",
    "\n",
    "- **(Optional) ACE adversity group** (`ace_group`): Using total ACE score (`aces_sum_score`). For example:\n",
    "  - Low ACE: 0 adverse events,\n",
    "  - Moderate ACE: 1–3 events,\n",
    "  - High ACE: ≥4 events.\n",
    "  \n",
    "  This follows common practice where ≥4 ACEs is often considered high risk.\n",
    "\n",
    "- **(Optional) Negative Urgency group** (`urgency_group`): Using the UPPS Negative Urgency score (`mh_y_upps__nurg_sum`). We can do a median split (High vs Low urgency) or use top quartile vs others depending on distribution. Here we'll do a median split for simplicity.\n",
    "\n",
    "- **(Optional) Externalizing behavior group** (`ext_group`): Using the CBCL externalizing syndrome score (`mh_p_cbcl__synd__ext_sum`). We will also do a median split into High vs Low externalizing behavior. (In practice, one might use clinical cutoffs if known; for teaching purposes, median split gives roughly equal groups.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cannabis use severity group\n",
    "def classify_mj_use(row):\n",
    "    days = row['su_y_tlfb__mj_ud']\n",
    "    any_use = row['su_y_mysu__use__mj__6mo_001']\n",
    "    if pd.isna(days):\n",
    "        # If we lack timeline data, fall back on any_use (if any_use is 1 or True, classify as moderate)\n",
    "        if not pd.isna(any_use) and any_use > 0:\n",
    "            return 1  # some use (moderate by default)\n",
    "        else:\n",
    "            return 0  # no use info -> assume non-user\n",
    "    else:\n",
    "        # If we have timeline days:\n",
    "        if days <= 0:\n",
    "            return 0  # 0 days -> non-user\n",
    "        elif days < 10:\n",
    "            return 1  # 1-9 days -> moderate use\n",
    "        else:\n",
    "            return 2  # >=10 days -> heavy use\n",
    "\n",
    "df['mj_group'] = df.apply(classify_mj_use, axis=1)\n",
    "\n",
    "# 2. Inhibitory control group (Flanker accuracy median split)\n",
    "median_acc = df['nc_y_flnkr__incongr_acc'].median(skipna=True)\n",
    "df['flanker_group'] = np.where(df['nc_y_flnkr__incongr_acc'] >= median_acc,\n",
    "                               \"High Inhibitory Control\",\n",
    "                               \"Low Inhibitory Control\")\n",
    "\n",
    "# 3. ACE groups (low:0, moderate:1-3, high:4+)\n",
    "def classify_ace(total_aces):\n",
    "    if pd.isna(total_aces):\n",
    "        return np.nan\n",
    "    total = int(total_aces)\n",
    "    if total <= 0:\n",
    "        return \"Low ACE (0)\"\n",
    "    elif total <= 3:\n",
    "        return \"Moderate ACE (1-3)\"\n",
    "    else:\n",
    "        return \"High ACE (4+)\"\n",
    "\n",
    "df['ace_group'] = df['aces_sum_score'].apply(classify_ace)\n",
    "\n",
    "# 4. Negative Urgency (UPPS) groups (median split)\n",
    "median_urg = df['mh_y_upps__nurg_sum'].median(skipna=True)\n",
    "df['urgency_group'] = np.where(df['mh_y_upps__nurg_sum'] >= median_urg,\n",
    "                               \"High Urgency\", \"Low Urgency\")\n",
    "\n",
    "# 5. Externalizing behavior groups (median split)\n",
    "median_ext = df['mh_p_cbcl__synd__ext_sum'].median(skipna=True)\n",
    "df['ext_group'] = np.where(df['mh_p_cbcl__synd__ext_sum'] >= median_ext,\n",
    "                            \"High Externalizing\", \"Low Externalizing\")\n",
    "\n",
    "# Quick sanity check: show value counts for the main grouping variables\n",
    "print(\"Cannabis use group counts: \", df['mj_group'].value_counts(dropna=False))\n",
    "print(\"Flanker inh. control group counts: \", df['flanker_group'].value_counts(dropna=False))\n",
    "print(\" ACE group counts: \", df['ace_group'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fafb54",
   "metadata": {},
   "source": [
    "In the above, the print statements will give us an overview of how many participants fall into each category (including NaN for those we cannot classify due to missing data). This helps ensure our groupings make sense (for example, we expect relatively few heavy cannabis users in an early-adolescent sample)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249d156",
   "metadata": {},
   "source": [
    "## Part 0.5: ROI Guide – The Brain's \"Brake System\" Mapped to Our Data\n",
    "\n",
    "Before diving into analysis, let's review the key components of the brain's \"braking\" circuitry and see how the variables in our dataset correspond to these components. We use a car brake metaphor:\n",
    "\n",
    "### Brake Pedal (Prefrontal Cortex regions: DLPFC, OFC, ACC)\n",
    "- **DLPFC (Dorsolateral Prefrontal Cortex)**: In charge of executive functions like working memory and planning; helps exert control by remembering goals (e.g., \"I want to stay sober\").\n",
    "- **OFC (Orbitofrontal Cortex)**: Evaluates consequences and reward vs. punishment (\"if I do this, what happens next?\"). In our metaphor, it weighs the immediate reward of pressing the gas vs the longer-term reward of braking.\n",
    "- **ACC (Anterior Cingulate Cortex)**: Monitors conflicts (detects when impulse and goal are at odds) and signals when we need to hit the brakes harder.\n",
    "\n",
    "Data perspective: These regions are the source of inhibitory signals. We do not have direct measures of DLPFC/OFC/ACC activity in this dataset (unless we had task fMRI for an inhibition task). However, behavioral performance on an inhibitory control task (the Flanker test accuracy, `nc_y_flnkr__incongr_acc`) reflects how well this \"brake pedal\" is functioning functionally. High accuracy suggests the PFC is effectively controlling attention and impulses (pressing the brake pedal well). If we had fMRI during an inhibition task, we'd expect to see these regions activate; if we had structural measures, chronic drug use can reduce their volume or activity (like \"worn-out brake pads\").\n",
    "\n",
    "### Brake Cables (White Matter Connections: Uncinate Fasciculus and Anterior Thalamic Radiation)\n",
    "- **Uncinate Fasciculus**: A white matter tract connecting the orbitofrontal cortex (OFC) with the anterior temporal lobe, including the amygdala. This link is crucial for regulating emotional responses — essentially routing the \"calm down, don't go for it\" message to the emotion and reward centers.\n",
    "- **Anterior Thalamic Radiation (ATR)**: White matter fibers connecting the prefrontal cortex (especially dorsomedial PFC) with the thalamus and striatum. This is a key part of the fronto-striatal \"No-Go\" pathway that implements inhibitory control. You can think of it as the main brake line carrying signals to the striatal brake mechanism.\n",
    "\n",
    "Data perspective: We have fractional anisotropy (FA) measures for these tracts:\n",
    "  - Left/Right uncinate FA (`mr_y_dti__fs__fa__at__unc__lh_wmean`, `mr_y_dti__fs__fa__at__unc__rh_wmean`),\n",
    "  - Left/Right ATR FA (`mr_y_dti__fs__fa__at__atr__lh_wmean`, `mr_y_dti__fs__fa__at__atr__rh_wmean`).\n",
    "FA is an index of white matter integrity: higher FA generally means a more coherent, well-myelinated tract, which could indicate a \"strong brake cable\" (signals transmit efficiently). Lower FA might suggest microstructural issues (fraying or weakening of the cable) which have been observed in substance users for these tracts (e.g., chronic drug use is associated with reduced FA in fronto-striatal pathways, implying a compromised connection between PFC and the striatum). In our analysis, we'll pay special attention to these measures.\n",
    "\n",
    "### Calipers (Striatum, e.g., Caudate and Putamen in the basal ganglia)\n",
    "These are the components that actually apply the brake by inhibiting actions. In the brain, the dorsal striatum (particularly the caudate and putamen) along with other basal ganglia structures (subthalamic nucleus, globus pallidus) implement the \"No-Go\" signal from the PFC. When the PFC sends a stop command, the striatal \"brake calipers\" engage to suppress the motor impulse or the tendency to seek a reward. The nucleus accumbens (ventral striatum) is also part of this circuit, especially for reward-related actions, acting like a junction between \"go\" and \"stop\" signals.\n",
    "\n",
    "Data perspective: We do not have direct DTI measures of the striatum (since FA is for white matter, and the striatum is gray matter), but we do have some fMRI task activation measures in striatal regions. For instance, our dataset includes beta coefficients for fMRI contrasts in:\n",
    "  - Left/Right caudate (`...__aseg__cd__lh_beta`, `__cd__rh_beta`),\n",
    "  - Left/Right putamen (`...__aseg__pt__lh_beta`, `__pt__rh_beta`),\n",
    "  - Left/Right nucleus accumbens (abbreviated as `aseg__ab__lh_beta`, which likely stands for accumbens basal area).\n",
    "These came from the Monetary Incentive Delay (MID) task (a reward task) in the dataset, not an inhibition task, but they indicate striatal reactivity to reward anticipation or outcomes. If we had a stop-signal task, we'd examine striatal activation during successful stopping. For now, keep in mind the striatum is the end of the brake circuit — even a strong PFC signal won't stop behavior if the \"calipers\" fail to clamp down. In addiction, studies often show reduced activation of striatal brakes during inhibition tasks and/or overactive striatum in response to drug cues.\n",
    "\n",
    "### Structural \"Scaffolding\"\n",
    "Apart from the direct brake cables, the brain has other structural connections that support overall communication. Two notable ones:\n",
    "- **Forceps Minor**: The anterior part of the corpus callosum connecting left and right frontal lobes. This is like the crossbeam that keeps the left and right brake systems in sync. If the corpus callosum (forceps minor) integrity is low, the hemispheres might not coordinate well in braking.\n",
    "- **Cingulum Bundle**: A white matter tract running along the limbic system (with parts connecting the cingulate cortex to the hippocampus and other regions). This can be thought of as a supporting route for communication between the frontal lobe (especially the ACC/cingulate region) and memory/emotion centers. It's not the primary brake line, but it provides an alternate route and structural support for cognitive control and memory integration.\n",
    "\n",
    "Data perspective: We included these in `structural_scaffold_vars`:\n",
    "  - Forceps minor FA (`mr_y_dti__fs__fa__at__fmin_wmean`),\n",
    "  - Cingulum cingulate part FA (left/right `__cgc__lh_wmean`, `__cgc__rh_wmean`),\n",
    "  - Cingulum hippocampal part FA (left/right `__cgh__lh_wmean`, `__cgh__rh_wmean`).\n",
    "These measures tell us about the integrity of the broader structural network. They might or might not be directly involved in \"braking\" behavior, but they could affect overall cognitive function. For example, a low cingulum FA might relate to attentional problems or mood regulation issues (since it connects to limbic structures).\n",
    "\n",
    "In summary, our dataset allows us to examine the \"brake system\" at multiple levels:\n",
    "- Functionally via behavior (Flanker accuracy) and possibly fMRI activation,\n",
    "- Structurally via white matter integrity (FA) in key tracts.\n",
    "\n",
    "Next, we'll proceed to an analysis example focusing on one piece of this system: the structural brake cable (white matter tract FA) and how it differs by a behavioral grouping (cannabis use severity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ecce7",
   "metadata": {},
   "source": [
    "## Part 1: ANOVA Example – Does White Matter \"Brake Cable\" Integrity Differ by Cannabis Use?\n",
    "\n",
    "In this section, we'll perform a one-way ANOVA to test whether the integrity of a key \"brake cable\" (the left anterior thalamic radiation white matter tract) differs among groups of youth with different levels of cannabis use (Non-users, Moderate users, Heavy users).\n",
    "\n",
    "Rationale: Chronic substance use, even in adolescence, might affect brain development. Our hypothesis (based on the brake system model) is that heavy cannabis use could be associated with weaker brake cables – i.e., lower fractional anisotropy in fronto-striatal white matter – reflecting possible microstructural degradation or delayed development of these tracts. Non-users are expected to have the highest FA (intact cables), heavy users the lowest, and moderate users perhaps intermediate.\n",
    "\n",
    "We'll go through these steps:\n",
    "1. **Data selection and grouping**: Focus on the later wave of data (when participants are older and some have started using cannabis). Define the groups (0, 1, 2) as \"Non-user,\" \"Moderate,\" \"Heavy.\"\n",
    "2. **Exploratory visualization**: Plot the distribution of left ATR FA for each cannabis group (e.g., boxplot) to see group means and variability.\n",
    "3. **Assumption checks**: ANOVA assumes (a) roughly normal distribution of the outcome in each group, (b) homogeneity of variances across groups, and (c) independent observations. We'll check normality informally (with plots or noting sample size) and test variance homogeneity with Levene's test. Independence is given since each subject is only in one group (we'll use only one time-point per subject).\n",
    "4. **ANOVA test**: Compute the F-statistic and p-value to see if there's a significant difference in mean FA between the groups.\n",
    "5. **Post-hoc comparisons**: If ANOVA is significant, do pairwise comparisons (t-tests between each pair of groups) and apply a Holm correction for multiple testing to control family-wise error rate. Identify which specific group differences are significant.\n",
    "6. **Interpretation**: Explain the findings in plain language and relate back to the brake metaphor.\n",
    "\n",
    "Let's execute this step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee71f4b",
   "metadata": {},
   "source": [
    "### Step 1: Prepare data for ANOVA\n",
    "\n",
    "We'll use the later wave of data (wave 21) to capture the adolescence period where cannabis use is observed. We already have a `mj_group` variable from Part 0 (0/1/2). We will filter the DataFrame to include only wave 21 and drop any missing values in the variables of interest (group or outcome) for a clean analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11981350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to the later time point (wave 21) for cross-sectional analysis\n",
    "df_analysis = df[df['wave'] == 21].copy()\n",
    "\n",
    "# Drop cases with no data for the outcome or grouping variable\n",
    "outcome_var = 'mr_y_dti__fs__fa__at__atr__lh_wmean'  # left ATR FA\n",
    "df_analysis = df_analysis.dropna(subset=[outcome_var, \"mj_group\"])\n",
    "\n",
    "# For nicer plotting, map group codes to labels\n",
    "group_labels = {0: \"Non-User\", 1: \"Moderate\", 2: \"Heavy\"}\n",
    "df_analysis['mj_group_label'] = df_analysis['mj_group'].map(group_labels)\n",
    "\n",
    "# Ensure the group label is categorical with a fixed order: Non-User, Moderate, Heavy\n",
    "df_analysis['mj_group_label'] = pd.Categorical(\n",
    "    df_analysis['mj_group_label'],\n",
    "    categories=['Non-User', 'Moderate', 'Heavy'],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Check how many in each group at wave 21\n",
    "print(\"Group counts at wave 21:\")\n",
    "print(df_analysis['mj_group_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9616e6d7",
   "metadata": {},
   "source": [
    "The above will display the number of Non-users, Moderate users, and Heavy users in our analysis sample. We expect Non-user to be by far the largest group (most teens have not used cannabis by this age in the study), and Heavy to be the smallest. This imbalance is okay, but we will keep it in mind when interpreting results (very small *N* in a group can make estimates less reliable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c39c0a1",
   "metadata": {},
   "source": [
    "### Step 2: Visualization of group distributions\n",
    "\n",
    "Let's create a boxplot (with jittered data points) of left ATR FA by cannabis group. This will show median FA and spread for each group, and any potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca358ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x='mj_group_label', y=outcome_var, data=df_analysis, width=0.6, palette=\"pastel\")\n",
    "sns.stripplot(x='mj_group_label', y=outcome_var, data=df_analysis, color='gray', size=4, alpha=0.6, jitter=0.2)\n",
    "plt.title(\"Left ATR FA by Cannabis Use Group\")\n",
    "plt.xlabel(\"Cannabis Use Severity Group\")\n",
    "plt.ylabel(\"Fractional Anisotropy (Left ATR)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f85ed",
   "metadata": {},
   "source": [
    "This plot gives a first impression:\n",
    "- Do the groups look different in terms of median FA?\n",
    "- Is the variability similar or very different across groups?\n",
    "- Any outliers or non-normal patterns (skew) visible?\n",
    "\n",
    "Expected observation: We might see that the Heavy users have a somewhat lower median FA and possibly a wider spread (if heavy use affects some individuals strongly). Non-users could have slightly higher median FA. Moderate users might overlap largely with Non-users (depending on how much use qualifies as moderate). Since this is synthetic data, the differences might or might not be visually obvious, but let's assume a trend of Non-user > Moderate > Heavy in FA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d6c41d",
   "metadata": {},
   "source": [
    "### Step 3: Assumption checks\n",
    "\n",
    "Normality: With a decent sample size in each group (likely hundreds of Non-users, and some dozens in Moderate/Heavy), the ANOVA is robust to moderate deviations from normality (Central Limit Theorem ensures group means are approximately normal). If needed, we could examine a histogram or Q–Q plot of residuals later. For now, we'll proceed assuming approximate normality.\n",
    "\n",
    "Homogeneity of variances: We will use Levene's test to statistically check if the variance of FA is equal across the three groups.\n",
    "\n",
    "Independence: Our data is one observation per participant at a single time, so independence is satisfied (no repeated measures in the analysis dataset). We just need to mention it and ensure we indeed filtered out multiple waves per person. We've already filtered to wave 21, so each subject appears at most once here. We can double-check no duplicate subject IDs in `df_analysis` by running:\n",
    "\n",
    "`print(\"Unique subjects in analysis:\", df_analysis['subject_id'].nunique(), \"out of total records:\", len(df_analysis))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d990dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare group samples for Levene's test\n",
    "fa_values = df_analysis[outcome_var]\n",
    "group_values = df_analysis['mj_group_label']\n",
    "\n",
    "# Levene's test for equal variances\n",
    "levene_stat, levene_p = stats.levene(\n",
    "    fa_values[group_values == \"Non-User\"],\n",
    "    fa_values[group_values == \"Moderate\"],\n",
    "    fa_values[group_values == \"Heavy\"],\n",
    "    center='median'\n",
    ")  # using median can be more robust\n",
    "print(f\"Levene's test p-value = {levene_p:.3f}\")\n",
    "\n",
    "# Double-check independence (no duplicate subject IDs)\n",
    "print(\"Unique subjects in analysis:\", df_analysis['subject_id'].nunique(),\n",
    "      \"out of total records:\", len(df_analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23341d9",
   "metadata": {},
   "source": [
    "We interpret Levene's test:\n",
    "- If p ≥ .05: We do not have evidence to reject equality of variances. Good — ANOVA's equal variance assumption holds.\n",
    "- If p < .05: The variances differ significantly between groups. In that case, a regular ANOVA might be slightly invalid; however, with unequal sample sizes, we could consider a Welch's ANOVA (which doesn't assume equal variances) or be cautious in interpreting results. For the sake of this lab, if this occurs we will note it but still proceed with the standard ANOVA (since ANOVA is somewhat robust, and we can double-check with Welch as needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1eaecb",
   "metadata": {},
   "source": [
    "### Step 4: Run the one-way ANOVA\n",
    "\n",
    "We will use a one way ANOVA F test to compare mean ATR FA across the three cannabis use groups.\n",
    "\n",
    "The F statistic compares the variation between the group means to the variation within the groups.\n",
    "\n",
    "- If the p value is less than 0.05, we have evidence that at least one group mean differs from the others.\n",
    "- If the p value is larger than 0.05, we do not have evidence of a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.4: Run the one way ANOVA using scipy\n",
    "\n",
    "# Collect the outcome values for each cannabis use group\n",
    "groups = []\n",
    "# Using the labels defined in Step 1: 'Non-User', 'Moderate', 'Heavy'\n",
    "target_labels = ['Non-User', 'Moderate', 'Heavy']\n",
    "\n",
    "for g in target_labels:\n",
    "    group_values = df_analysis.loc[df_analysis['mj_group_label'] == g, outcome_var].dropna()\n",
    "    groups.append(group_values)\n",
    "\n",
    "# Run one way ANOVA F test\n",
    "F_stat, p_val = stats.f_oneway(*groups)\n",
    "\n",
    "# Degrees of freedom\n",
    "k = len(target_labels)                    # number of groups\n",
    "N = sum(len(g) for g in groups)         # total sample size across groups\n",
    "df_between = k - 1\n",
    "df_within = N - k\n",
    "\n",
    "print(f\"One way ANOVA: F({df_between}, {df_within}) = {F_stat:.2f}, p = {p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb364b",
   "metadata": {},
   "source": [
    "If the p-value is significant (p < .05), it means there is a statistically reliable difference in mean left ATR FA among the three groups. If p is larger (> .05), we conclude there's no evidence of group differences in this sample.\n",
    "\n",
    "**Interpretation of our result:**\n",
    "The ANOVA resulted in **F(2, 9281) = 0.37, p = 0.6936**.\n",
    "Since the p-value (0.6936) is much larger than 0.05, we **fail to reject the null hypothesis**. We do not have evidence that the mean fractional anisotropy (FA) of the left ATR differs across the three cannabis use groups. The groups appear to have similar means.\n",
    "\n",
    "Typically, if the overall ANOVA is not significant, we stop there. However, for the sake of demonstration, we will proceed to the pairwise comparisons in the next step to see how the code works, even though we expect no significant differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9eeb25",
   "metadata": {},
   "source": [
    "### Step 5: Post-hoc pairwise comparisons with Holm correction\n",
    "\n",
    "We have 3 groups, so there are 3 pairwise comparisons to test:\n",
    "- Non-user vs Moderate\n",
    "- Non-user vs Heavy\n",
    "- Moderate vs Heavy\n",
    "\n",
    "We'll perform independent-samples t-tests for each pair. Because we already saw potential unequal variances and unequal sample sizes, we'll use Welch's t-test (i.e., not assuming equal variances) for safety. Then we'll apply the Holm–Bonferroni correction to the three p-values to control the family-wise error rate at α = 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62979e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise t-tests between groups\n",
    "group_names = ['Non-User', 'Moderate', 'Heavy']\n",
    "pairs = [(\"Non-User\", \"Moderate\"), (\"Non-User\", \"Heavy\"), (\"Moderate\", \"Heavy\")]\n",
    "p_vals = []\n",
    "for g1, g2 in pairs:\n",
    "    data1 = df_analysis.loc[df_analysis['mj_group_label'] == g1, outcome_var].dropna()\n",
    "    data2 = df_analysis.loc[df_analysis['mj_group_label'] == g2, outcome_var].dropna()\n",
    "    t_stat, p_val = stats.ttest_ind(data1, data2, equal_var=False,\n",
    "                                    nan_policy='omit')\n",
    "    p_vals.append(p_val)\n",
    "    print(f\"{g1} vs {g2}: t={t_stat:.2f}, p={p_val:.4f}\")\n",
    "\n",
    "# Holm–Bonferroni correction for the 3 p-values\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "reject, p_vals_holm, _, _ = multipletests(p_vals, alpha=0.05, method='holm')\n",
    "pairwise_results = pd.DataFrame({\n",
    "    \"Comparison\": [f\"{g1} vs {g2}\" for g1, g2 in pairs],\n",
    "    \"raw p\": p_vals,\n",
    "    \"Holm-adjusted p\": p_vals_holm,\n",
    "    \"Significant (Holm 0.05)\": reject\n",
    "})\n",
    "\n",
    "print(\"Pairwise comparison p-values (Holm-corrected):\")\n",
    "print(pairwise_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf37f12",
   "metadata": {},
   "source": [
    "The printouts will show:\n",
    "- Raw p-values for each comparison,\n",
    "- Holm-adjusted p-values, and whether each comparison is significant after correction.\n",
    "\n",
    "**Interpretation of our result:**\n",
    "The pairwise comparisons confirm the ANOVA result.\n",
    "- Non-User vs Moderate: p ≈ 0.38\n",
    "- Non-User vs Heavy: p ≈ 0.92\n",
    "- Moderate vs Heavy: p ≈ 0.40\n",
    "\n",
    "After Holm correction, all adjusted p-values are **1.0**. None of the comparisons are significant. There is no statistical evidence that any specific group differs from another in terms of left ATR FA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa47b305",
   "metadata": {},
   "source": [
    "### Step 6: Interpretation of results\n",
    "\n",
    "Based on the actual results obtained above:\n",
    "\n",
    "- **The overall ANOVA was not significant** (p = 0.69), indicating no detectable differences in the integrity of the left Anterior Thalamic Radiation (ATR) across the Non-user, Moderate, and Heavy cannabis use groups.\n",
    "- **Post-hoc tests confirmed this**, showing no significant differences between any pair of groups.\n",
    "\n",
    "**Conclusion:**\n",
    "In this specific analysis of the synthetic ABCD dataset, we did **not** find evidence to support the hypothesis that cannabis use severity is associated with reduced integrity of the \"brake cable\" (left ATR white matter). The \"brake cables\" appear to be equally intact across all three groups.\n",
    "\n",
    "**Why might this be?**\n",
    "1.  **Synthetic Data:** This is a synthetic dataset. The relationships programmed into it might not perfectly mirror the complex, subtle effects found in real-world biological data.\n",
    "2.  **Effect Size:** Even in real data, effects of substance use on brain structure can be small and difficult to detect without very specific controls or longitudinal designs.\n",
    "3.  **Resilience:** It is possible that in this sample, the \"Heavy\" users have not used cannabis long enough or heavily enough to cause structural white matter changes, or that this specific tract (left ATR) is resilient.\n",
    "\n",
    "**Scientific Implication:**\n",
    "While we didn't find a \"dose-dependent\" degradation of the brake cable here, a null result is still a result! It tells us that, at least for this variable and this grouping, cannabis use is not a differentiating factor. In a real study, we might next look at other tracts (like the Uncinate Fasciculus), other behavioral measures (like Flanker performance), or look for interactions with other variables (e.g., maybe cannabis only affects the brake cable in youth who also have high life stress)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc70c0",
   "metadata": {},
   "source": [
    "## Part 2: Your Turn – ANOVA on Another \"Brake Cable\"\n",
    "\n",
    "Now it’s your turn to apply ANOVA to a different brain \"brake cable.\" Choose one outcome variable from the remaining brake cable measures and one grouping variable to explore. Follow the same steps as in Part 1: create a 3-level grouping (Low/Medium/High or None/Moderate/Heavy), visualize the data, run the ANOVA, and interpret the result.\n",
    "\n",
    "### Options (Choose ONE combination)\n",
    "\n",
    "**Option A: Continue the Demo (Same Grouping, Different Tract)**\n",
    "- **Grouping:** Cannabis use severity (already created as `mj_group_label`).\n",
    "- **Outcome:** Choose a different structural tract, e.g., Right ATR (`...atr__rh_wmean`) or Uncinate Fasciculus (`...unc__lh_wmean`).\n",
    "- **Question:** Does cannabis use affect other parts of the brake system?\n",
    "\n",
    "**Option B: New Grouping, New Outcome**\n",
    "- **Outcome Variable (Choose one):**\n",
    "    - **Functional connectivity:** FPN–Caudate connectivity (left or right) – `mr_y_rsfmri__corr__gpnet__aseg__frp__cd__lh_mean` or `..._rh_mean`\n",
    "    - **Structural connectivity (FA):** Uncinate fasciculus FA (left or right) – `mr_y_dti__fs__fa__at__unc__lh_wmean` or `..._rh_wmean`\n",
    "    - **Structural connectivity (FA):** Forceps minor FA – `mr_y_dti__fs__fa__at__fmin_wmean`\n",
    "    - **Structural connectivity (FA):** Cingulum bundle FA (left or right) – `mr_y_dti__fs__fa__at__cgc__lh_wmean` or `..._rh_wmean`\n",
    "\n",
    "- **Grouping Variable (Choose one):**\n",
    "    - **Inhibitory control accuracy** – Flanker incongruent accuracy `nc_y_flnkr__incongr_acc`.\n",
    "    - **Negative urgency (impulsivity trait)** – `mh_y_upps__nurg_sum`.\n",
    "    - **Externalizing problems** – `mh_p_cbcl__synd__ext_sum`.\n",
    "    - **Childhood adversity (ACE score)** – `aces_sum_score`.\n",
    "\n",
    "*Note:* If you choose a new grouping variable (Option B), you will need to bin it into 3 groups (Low/Medium/High) using `pd.qcut` or similar logic.\n",
    "\n",
    "---\n",
    "\n",
    "### Workflow for Your Turn\n",
    "\n",
    "1.  **Define Groups and Outcome:** Select your variables, handle missing data (e.g., 999, 777), and create your 3-level grouping variable.\n",
    "2.  **Descriptive Statistics:** Calculate the mean and SD for each group.\n",
    "3.  **Plot 1 (Assumption Check):** Create overlapping histograms to check distributions.\n",
    "4.  **Plot 2 (Main Result):** Create a boxplot or bar plot to visualize group differences.\n",
    "5.  **ANOVA Test:** Run the one-way ANOVA using `scipy.stats.f_oneway`.\n",
    "6.  **Post-hoc Tests (if needed):** If ANOVA is significant (or just for practice), run pairwise t-tests with Holm correction.\n",
    "7.  **Interpretation:** Interpret your findings in the context of the \"brake system.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b08b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DEFINE GROUPS AND OUTCOME\n",
    "# ----------------------------\n",
    "# Goal: Select your variables, clean missing data, and create the grouping column.\n",
    "\n",
    "df_part2 = df[df['wave'] == 21].copy()\n",
    "\n",
    "# TODO: Set your chosen variables\n",
    "# outcome_var_2 = '...' \n",
    "# group_var_2 = '...'   \n",
    "\n",
    "# TODO: Handle missing codes (e.g., 999, 777) for your chosen variables\n",
    "# df_part2.loc[..., ...] = np.nan\n",
    "\n",
    "# TODO: Create grouping variable (if not using existing mj_group_label)\n",
    "# Hint: Use pd.qcut(df_part2[group_var_2], 3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "# df_part2['group_label'] = ...\n",
    "\n",
    "# TODO: Drop rows with missing data in outcome or group\n",
    "# df_part2 = ...\n",
    "\n",
    "# Check counts\n",
    "# print(df_part2['group_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d83285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DESCRIPTIVE STATISTICS\n",
    "# -------------------------\n",
    "# Compute mean and SD for each group\n",
    "\n",
    "# TODO: Calculate and print means/SDs using groupby\n",
    "# group_stats = df_part2.groupby('group_label')[outcome_var_2].agg(['mean', 'std'])\n",
    "# print(group_stats)\n",
    "\n",
    "# TODO: Print overall statistics\n",
    "# print(f\"\\nOverall mean: {df_part2[outcome_var_2].mean():.4f}\")\n",
    "# print(f\"Overall SD: {df_part2[outcome_var_2].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff98e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PLOT 1: ASSUMPTION CHECK (HISTOGRAMS)\n",
    "# ----------------------------------------\n",
    "# Visualize distributions to check for normality and equal variance\n",
    "\n",
    "# TODO: Create overlapping histograms\n",
    "plt.figure(figsize=(8, 5))\n",
    "# Hint: Loop through unique group labels\n",
    "# for label in df_part2['group_label'].unique():\n",
    "#     subset = df_part2[df_part2['group_label'] == label]\n",
    "#     sns.histplot(subset[outcome_var_2], kde=True, label=label, alpha=0.4)\n",
    "plt.legend()\n",
    "plt.title(\"Distribution by Group\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PLOT 2: MAIN RESULT (BOXPLOT)\n",
    "# --------------------------------\n",
    "# Visualize the group differences\n",
    "\n",
    "# TODO: Create a boxplot\n",
    "plt.figure(figsize=(6, 4))\n",
    "# sns.boxplot(x='group_label', y=outcome_var_2, data=df_part2, palette=\"pastel\")\n",
    "# sns.stripplot(x='group_label', y=outcome_var_2, data=df_part2, color='gray', alpha=0.5, jitter=0.2)\n",
    "plt.title(\"Outcome by Group\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6223216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ANOVA TEST\n",
    "# -------------\n",
    "# Run the one-way ANOVA\n",
    "\n",
    "# TODO: Run the ANOVA\n",
    "# Hint: Pass the data for each group to stats.f_oneway\n",
    "# You can select them manually: df_part2[df_part2['group_label'] == 'Low'][outcome_var_2]\n",
    "# (Make sure to use the correct group labels for your choice!)\n",
    "\n",
    "# f_stat, p_val = stats.f_oneway(\n",
    "#     df_part2[df_part2['group_label'] == 'Low'][outcome_var_2].dropna(),\n",
    "#     df_part2[df_part2['group_label'] == 'Medium'][outcome_var_2].dropna(),\n",
    "#     df_part2[df_part2['group_label'] == 'High'][outcome_var_2].dropna()\n",
    "# )\n",
    "\n",
    "# # Degrees of freedom\n",
    "# k = 3  # number of groups\n",
    "# N = len(df_part2)\n",
    "# df_between = k - 1\n",
    "# df_within = N - k\n",
    "\n",
    "# print(f\"\\nOne-way ANOVA results: F({df_between}, {df_within}) = {f_stat:.4f}, p = {p_val:.4f}\")\n",
    "\n",
    "# if p_val < 0.05:\n",
    "#     print(\"Result: Significant difference detected between groups (p < 0.05)\")\n",
    "# else:\n",
    "#     print(\"Result: No significant difference between groups (p >= 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. POST-HOC COMPARISONS\n",
    "# -----------------------\n",
    "# If the ANOVA is significant, run pairwise t-tests with Holm-Bonferroni correction\n",
    "\n",
    "# TODO: Define the pairs based on your group labels\n",
    "# pairs = [('Low', 'Medium'), ('Low', 'High'), ('Medium', 'High')] \n",
    "# (Or 'Non-User', 'Moderate', 'Heavy' if using Option A)\n",
    "\n",
    "# TODO: Run pairwise t-tests\n",
    "# p_values = []\n",
    "# for g1, g2 in pairs:\n",
    "#     data1 = df_part2[df_part2['group_label'] == g1][outcome_var_2].dropna()\n",
    "#     data2 = df_part2[df_part2['group_label'] == g2][outcome_var_2].dropna()\n",
    "#     t_stat, p = stats.ttest_ind(data1, data2, equal_var=False)\n",
    "#     p_values.append(p)\n",
    "\n",
    "# TODO: Apply Holm-Bonferroni correction\n",
    "# reject, p_corrected, _, _ = multipletests(p_values, alpha=0.05, method='holm')\n",
    "\n",
    "# # Print results\n",
    "# print(\"\\nPost-hoc Pairwise Comparisons (Holm-Bonferroni corrected):\")\n",
    "# for i, (g1, g2) in enumerate(pairs):\n",
    "#     print(f\"{g1} vs {g2}: p_raw = {p_values[i]:.4f}, p_corrected = {p_corrected[i]:.4f} ({'Significant' if reject[i] else 'Not Significant'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ddd513",
   "metadata": {},
   "source": [
    "### 7. Interpretation\n",
    "\n",
    "**Question:** Based on your ANOVA and post-hoc results (if applicable), what can you conclude about the relationship between your chosen grouping variable and the outcome variable?\n",
    "\n",
    "*Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f85e9",
   "metadata": {},
   "source": [
    "## Part 3: Multiple Testing Correction\n",
    "\n",
    "By now, you have conducted two ANOVA tests: one in Part 1 and one in Part 2. When we perform multiple statistical tests, the chance of a false-positive (Type I error) increases. In this section, we will apply multiple comparisons corrections to the p-values from the two ANOVAs to control the family-wise error rate and the false discovery rate (FDR).\n",
    "\n",
    "Why adjust for multiple tests? If we had an $\\alpha = 0.05$ significance level for each test, running multiple independent tests actually gives a higher overall probability of wrongly rejecting at least one true null hypothesis. Corrections like Bonferroni and Holm control the family-wise error rate (probability of any false positive) in different ways, while the Benjamini–Hochberg (BH) procedure controls the expected proportion of false positives (FDR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de788fb9",
   "metadata": {},
   "source": [
    "Step 1: Collect your p-values from Part 1 and Part 2. Let’s call them `p1` and `p2`. Plug them into the list below (replace the example values with your actual results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea9fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from statsmodels.stats import multitest\n",
    "# Replace these example p-values with your actual ANOVA p-values:\n",
    "p_values = [0.04, 0.7791]\n",
    "# [p1, p2]\n",
    "# 1. Bonferroni correction\n",
    "reject_bonf, pvals_bonf, _, _ = multitest.multipletests(p_values, alpha=0.05, method='bonferroni')\n",
    "# 2. Holm correction\n",
    "reject_holm, pvals_holm, _, _ = multitest.multipletests(p_values, alpha=0.05, method='holm')\n",
    "# 3. Benjamini-Hochberg FDR correction\n",
    "reject_bh, pvals_bh, _, _ = multitest.multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "# Organize results into a table\n",
    "results_table = pd.DataFrame({\n",
    "    'Test': ['Part1 ANOVA', 'Part2 ANOVA'],\n",
    "    'Raw p': p_values,\n",
    "    'Bonferroni Sig?': ['Yes' if r else 'No' for r in reject_bonf],\n",
    "    'Holm Sig?': ['Yes' if r else 'No' for r in reject_holm],\n",
    "    'BH Sig?': ['Yes' if r else 'No' for r in reject_bh]\n",
    "})\n",
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f776b",
   "metadata": {},
   "source": [
    "The code above will output a table like below (using the example p-values):\n",
    "\n",
    "| Test | Raw p | Bonferroni Sig? | Holm Sig? | BH Sig? |\n",
    "|-----|-------|-----------------|-----------|---------|\n",
    "| Part1 ANOVA | 0.0400 | No | No | No |\n",
    "| Part2 ANOVA | 0.7791 | No | No | No |\n",
    "\n",
    "How to read this table:\n",
    "\n",
    "- **Raw p**: the original p-value from each ANOVA. In our example, Part1’s test had p = 0.04 (just below 0.05), while Part2 was 0.779.\n",
    "- **Bonferroni Sig?**: “Yes” if the test is significant after Bonferroni correction. Bonferroni is very strict; it multiplies p-values by the number of tests or equivalently uses a stricter threshold. In the example, Part1’s p (0.04) is not below the corrected threshold, so it is no longer significant after Bonferroni correction.\n",
    "- **Holm Sig?**: “Yes” if significant after Holm’s stepwise correction. Holm’s method is slightly less conservative than Bonferroni.\n",
    "- **BH Sig?**: “Yes” if significant after Benjamini–Hochberg FDR procedure (at FDR = 0.05). This method is more lenient; it allows a controlled proportion of false positives.\n",
    "\n",
    "Takeaway: If you had multiple tests and one came out just barely significant (like 0.04), a Bonferroni correction might render it non-significant when considering the family of tests. Always report whether your findings hold after such corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce27b7e",
   "metadata": {},
   "source": [
    "## Optional Extension: Missing Data in Neuroimaging Studies\n",
    "\n",
    "In this lab, we have been working with the `labs/data/L1/ABCD_synthetic.csv` dataset. You might recall that at the very beginning, we replaced missing data codes like 777 and 999 with `NaN` (Not a Number). In neuroimaging studies, missing data is extremely common. Participants might move too much in the scanner, feel anxious and stop the scan, or not show up for the MRI visit at all.\n",
    "\n",
    "When data is missing, it falls into one of three conceptual categories:\n",
    "\n",
    "1.  **Missing Completely at Random (MCAR):** The missingness has nothing to do with the participant's characteristics.\n",
    "    *   *Example:* A file server error accidentally deletes 5% of the brain scans at random.\n",
    "2.  **Missing at Random (MAR):** The missingness is related to observed data we have collected.\n",
    "    *   *Example:* Younger children move more in the scanner, leading to more unusable data. If we have measured \"age,\" we can explain the missingness.\n",
    "3.  **Missing Not at Random (MNAR):** The missingness is related to the unobserved data itself or unmeasured variables.\n",
    "    *   *Example:* Participants with the highest levels of impulsivity or substance craving are the most likely to skip the MRI session, and we don't have other measures of that craving.\n",
    "\n",
    "**Why does this matter?**\n",
    "In fMRI and DTI studies, missing data is rarely MCAR. It is frequently patterned by sociodemographic factors (like income, education, or race) and behavioral traits. If we simply drop everyone with missing data (Listwise Deletion), our final sample might be \"healthier\" or more advantaged than the real population. This biases our results.\n",
    "\n",
    "While advanced methods like **Inverse Probability Weighting (IPW)**, **Multiple Imputation**, or **Full Information Maximum Likelihood (FIML)** can help address this, they are complex. In this extension, we will use simple descriptive statistics and hypothesis tests to *detect* if our missingness is patterned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eeab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Create Missingness Flags\n",
    "# --------------------------------\n",
    "\n",
    "# 1. Define flags for having complete structural or functional brake data\n",
    "# We use .notna().all(axis=1) to check if a row has non-missing values for ALL variables in the list\n",
    "df['has_structural_brake'] = df[structural_brake_vars].notna().all(axis=1).astype(int)\n",
    "df['has_functional_brake'] = df[functional_brake_vars].notna().all(axis=1).astype(int)\n",
    "\n",
    "# 2. Print value counts and proportions\n",
    "print(\"--- Structural Brake Data Availability ---\")\n",
    "print(df['has_structural_brake'].value_counts())\n",
    "print(df['has_structural_brake'].value_counts(normalize=True))\n",
    "print(\"1 = Complete Data, 0 = Missing Data\\n\")\n",
    "\n",
    "print(\"--- Functional Brake Data Availability ---\")\n",
    "print(df['has_functional_brake'].value_counts())\n",
    "print(df['has_functional_brake'].value_counts(normalize=True))\n",
    "print(\"1 = Complete Data, 0 = Missing Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e968ed39",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "Look at the proportions above. In many real neuroimaging datasets, 10-20% or more of the data might be unusable due to motion or other artifacts. This confirms that we are analyzing a subset of our original sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9962319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Hypothesis Tests for Missingness Patterns\n",
    "# -------------------------------------------------\n",
    "# We will check if missingness in structural brake data is related to demographics.\n",
    "\n",
    "# A. Continuous Variable Check (e.g., Age or ACEs)\n",
    "# We'll try to find an age variable, otherwise use ACE scores.\n",
    "continuous_candidates = [col for col in df.columns if 'age' in col.lower() or 'income' in col.lower()]\n",
    "if not continuous_candidates:\n",
    "    continuous_candidates = ['aces_sum_score'] # Fallback\n",
    "\n",
    "target_cont = continuous_candidates[0]\n",
    "print(f\"Testing for differences in continuous variable: {target_cont}\")\n",
    "\n",
    "# Drop NaNs for the test\n",
    "valid_cont = df.dropna(subset=[target_cont, 'has_structural_brake'])\n",
    "\n",
    "# T-test\n",
    "group_has_data = valid_cont[valid_cont['has_structural_brake'] == 1][target_cont]\n",
    "group_missing_data = valid_cont[valid_cont['has_structural_brake'] == 0][target_cont]\n",
    "\n",
    "t_stat, p_val = stats.ttest_ind(group_has_data, group_missing_data)\n",
    "\n",
    "print(f\"Mean (With Data): {group_has_data.mean():.2f}\")\n",
    "print(f\"Mean (Missing Data): {group_missing_data.mean():.2f}\")\n",
    "print(f\"T-test: t={t_stat:.2f}, p={p_val:.4f}\\n\")\n",
    "\n",
    "\n",
    "# B. Categorical Variable Check (e.g., Sex)\n",
    "# We'll try to find a sex/gender variable.\n",
    "categorical_candidates = [col for col in df.columns if 'sex' in col.lower() or 'gender' in col.lower()]\n",
    "if not categorical_candidates:\n",
    "    print(\"Could not find a sex/gender variable automatically.\")\n",
    "else:\n",
    "    target_cat = categorical_candidates[0]\n",
    "    print(f\"Testing for differences in categorical variable: {target_cat}\")\n",
    "    \n",
    "    # Contingency Table\n",
    "    contingency_table = pd.crosstab(df['has_structural_brake'], df[target_cat])\n",
    "    print(\"\\nContingency Table:\")\n",
    "    print(contingency_table)\n",
    "    \n",
    "    # Chi-Square Test\n",
    "    chi2, p_val, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "    print(f\"Chi-Square Test: chi2={chi2:.2f}, p={p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ebf78",
   "metadata": {},
   "source": [
    "**Interpretation of Missingness Checks:**\n",
    "\n",
    "*   **Continuous Variable:** Did the two groups (With Data vs. Missing Data) differ significantly in age, income, or ACE scores? If $p < 0.05$, it suggests the data are not MCAR with respect to that variable.\n",
    "*   **Categorical Variable:** Was the proportion of missing data different across groups (e.g., males vs. females)? A significant Chi-square result suggests a pattern.\n",
    "\n",
    "If you found significant differences, this suggests our \"complete case\" analysis might be biased toward a specific demographic profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf300e",
   "metadata": {},
   "source": [
    "### Reflection: Who is missing from our brain data and why it matters\n",
    "\n",
    "**Think about it:**\n",
    "If youth from lower-income families or those with higher life adversity (ACEs) are less likely to have usable MRI data, how might that influence our conclusions about the \"typical\" adolescent brain? If we only study the most advantaged, stable youth, we might miss important effects of stress or environment on brain development.\n",
    "\n",
    "**Advanced Methods (For your awareness):**\n",
    "Researchers use several strategies to handle this, though we won't implement them here:\n",
    "1.  **Inverse Probability Weighting (IPW):** Giving more \"weight\" to the participants who *did* provide data but look like the ones who are missing (e.g., up-weighting the few low-income participants we have).\n",
    "2.  **Multiple Imputation:** Using statistical relationships between variables to fill in plausible values for the missing data, creating multiple \"complete\" datasets to analyze.\n",
    "3.  **Full Information Maximum Likelihood (FIML):** A mathematical way of estimating model parameters that uses *all* available data points for each person, rather than dropping anyone with a single missing value.\n",
    "\n",
    "**Takeaway:**\n",
    "Always check who is missing. Treat missingness as a signal, not just a nuisance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
